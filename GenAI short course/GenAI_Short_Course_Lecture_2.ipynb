{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZyvXSmxl3S"
      },
      "source": [
        "# Generative AI - Combient Mix AB / Silo AI for Patricia AB 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sF2Up6-RmhA"
      },
      "source": [
        "Prompt Engineering (PE) is the primary vehicle for guiding generative AI models towards stable applications. It is particularly applicable to Large Language Models (LLMs) and involves formulating prompts and prompting schemas in order to retrieve appropriate responses to queries. It is essential for meaningful interactions with LLMs.\n",
        "\n",
        "This notebook, which is the second of two notebooks, provides a hands-on introduction to PE for end users with a basic understanding of the Python programming language. No advanced coding or technical background knowledge of generative AI or LLM is required for completing the exercises in the notebook. The notebook is divided into three sections, each corresponding to material covered in two live seminars. There will be ~60 minutes available in order to go through and complete exercise 3.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "***\n",
        "\n",
        "**Structure**\n",
        "\n",
        "Exercise **1 & 2 during the first session** (~ 40 minutes per exercise) and exercise **3 during the second session** (~ 60 minutes)\n",
        "\n",
        "\n",
        "1.) First, we'll explore the basics of prompting LLMs and the importance of PE. We will see examples of some important and cutting edge techniques for prompting efficiently and with intent\n",
        "* 0 / 1 / few-shot prompting,\n",
        "* Role-Task-Format (RTF),\n",
        "* In-Context-Learning (ICL),\n",
        "* Chain-of-Thought (CoT) and\n",
        "* Tree-of-Thought (ToT)\n",
        "\n",
        "2.) We'll learn how to integrate and utilize existing tools for specific tasks. We focus on some of the tools available via enabling plugins for GPT-4 at [OpenAI chat interface](https://chat.openai.com/). Example tasks include \n",
        "  * **Searching the internet for recent information** LLMs are generally pre-trained and thus have a knowledge cutoff at the time when training stopped. The latest GPT-4 model of OpenAI for example has a knowledge cutoff at January 2022. This means that it was not trained on any data produced after that time, so if we want to include updated information this must be done by fetching the information and providing it as context together with our query.\n",
        "  * **Using your own PDF documents as a knowledge base** (plugin: AskYourPDF). There are several plugins available for GPT-4 which provide the capability of uploading a PDF document and use it as a reference for interacting with the LLM.\n",
        "\n",
        "3.) The third exercise is more programming heavy and involves working with API calls. We'll guide you through it! This allows us to perform more complex tasks and write our own custom functions utilizing the power of the LLM. We introduce the Python library [Langchain](https://python.langchain.com/docs/get_started/introduction.html) and some of its tools for manipulating prompts. This allows for more intricate problem-solving as well as gaining control over the reliability of the output. We will see how to use this in order to accomplish Retrieval Augmented Generation (RAG) from our own knowledge base (the PDF document). This exercise demonstrates how the plugins used in exercise 2 work under the hood. Examples of what we can achieve include\n",
        "\n",
        "* constraining the system behaviour, for example to mitigate hallucinations from the LLM\n",
        "* Retrieval Augmented Generation from an external knowledge base such as the internet or a collection of documents\n",
        "\n",
        "This provides a deeper understanding of the fundamental components essential for constructing advanced generative AI systems.\n",
        "\n",
        "***\n",
        "\n",
        "**Note:**\n",
        "\n",
        "In order to run this notebook properly you will need\n",
        "\n",
        "* **Gmail account** - Follow the provided instructions to download the notebook to your GDrive, so that you can edit and save it freely.\n",
        "* **Google GDrive access** - enable the app `Google Colaboratory`. The process is highlighted in the accompanying reading material, but you can also ask ChatGPT a question like this by posing the following query <span style=\"color:blue;\">how do I enable google colab for the first time?</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FflnrWlYmoZ"
      },
      "source": [
        "> Run the below code blocks to install necessary packages.\n",
        ">\n",
        "> Notebook code blocks can be executed via either: \n",
        "> * **shift + enter**: executes current code block and moves to the cell below\n",
        "> * **control + enter**: executes current code block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uczbePAtomja"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "Here we set up the environment and make sure we can access data via Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the code below to load the GenAI map we will be using during this course. The code is fetching a map called GenAI from our Github repository. Click on plus next to the cell to run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_drive = False\n",
        "\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd /content/drive/MyDrive\n",
        "\n",
        "  !git clone https://github.com/statisticalmodel/GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now view the files by clicking on the map and follow the path down in the GenAI short course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image of GitClone](https://github.com/statisticalmodel/GenAI/blob/main/GenAI%20short%20course/GitClone.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Packages & Imports\n",
        "\n",
        "Installing and importing necessary packages/modules. Note that these should be installed only in a virtual environment when using the Colab Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the below code block to install some of the Python libraries which are required for running the Notebook.\n",
        "\n",
        "Note that this may take up to ~15 seconds to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: All packages below are not necessary and should be cleaned after completion of exercises**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU transformers \\\n",
        "    --upgrade huggingface_hub \\\n",
        "    -q sentencepiece \\\n",
        "    -q tiktoken \\\n",
        "    -q openai \\\n",
        "    -q langchain \\\n",
        "    -q sentence-transformers \\\n",
        "    -q jq \\\n",
        "    -q faiss-cpu \\\n",
        "    -q pypdf \\\n",
        "    -q wikipedia \\\n",
        "    -q duckduckgo-search \\\n",
        "    -q colorama \\\n",
        "    -q PyMuPDF "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU openai \\\n",
        "    -q langchain \\\n",
        "    -q faiss-cpu \\\n",
        "    -q wikipedia \\\n",
        "    -q duckduckgo-search \\\n",
        "    -q colorama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the below code block to import the necessary library modules used in the notebook.\n",
        "\n",
        "Note that this may take up to ~20 seconds to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: All packages below are not necessaty and should be cleaned after completion of exercises**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some system and base modules\n",
        "import os\n",
        "import sys\n",
        "from timeit import default_timer as timer\n",
        "import getpass\n",
        "\n",
        "# NLP modules\n",
        "import openai\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# Other modules\n",
        "from colorama import Fore, Back, Style"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import helper functions used in the Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the code block below to import customized helper functions used in the notebook. These functions have been written to, for example, process PDF files and reduce clutter in the notebook by using explicit code. All of the imported functions are located in the file `helper_functions.py` where you may explore their definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/GenAI/GenAI short course/')\n",
        "\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting the access key for OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The OpenAI API key can be set manually in the notebook by running the code cell block below.**\n",
        "\n",
        "You can optionally set it as an environment variable; by typing the following in your Mac terminal\n",
        "```\n",
        "export OPENAI_API_KEY=sk-...\n",
        "```\n",
        "or if you are using windows 10 you can type the following in a Command window\n",
        "```\n",
        "set OPENAI_API_KEY=sk-...\n",
        "```\n",
        "Observe the lack of space in the value designations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: Delete the API access key below before pushing to repo etc.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sk-...\n",
        "\n",
        "# Here we can set the OpenAI API access key manually in case it fails to load from the environment.\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    api_key = getpass.getpass(\"Enter OpenAI API Key here\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "else:\n",
        "  print(f\"OPENAI_API_KEY fetched from environment!\")\n",
        "\n",
        "\n",
        "#print(f\"The Open AI access key is given by: \\n\\n {os.environ['OPENAI_API_KEY']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q97YM4YIncHT"
      },
      "source": [
        "# Hands-On 3: Customized Tools for Extended LLM Functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Interacting with ChatGPT via the OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiaTQss8n14Q"
      },
      "source": [
        "Example themes for exercises:\n",
        "\n",
        "* Search the internet using Wikipedia and DuckDuckGo.\n",
        "  - Useful and easy to implement and understand at a basic level.\n",
        "* Search a knowledge base, e.g. contained within a PDF.\n",
        "  - Useful for many purposes. Builds on previous exercises and showcase the behind the scenes work involved in the plugins in the OpenAI browser environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why should you use Python and API?**\n",
        "\n",
        "Using the OpenAI API (Application Program Interface) to access their models, instead of the online chat interface, gives us several advantages, especially when we want to build custom made solutions. One of the big perks of using the API is automation. Python, as a common programming langauge, is great at taking care of repeating or tricky tasks for us, which is helpful when we have a lot of such tasks to handle.\n",
        "\n",
        "With the help of python we can also get the OpenAI models to work well with other tools and software we have. Plus, it lets us adjust things to work the way we want, making our interaction with the OpenAI models more customs made.\n",
        "\n",
        "Python has a lot of advanced features which allow us to use the OpenAI services to their full potential. This means we can do a wider variety of tasks. In this part of the course, we will look into how a model can get access to more recent data via accessing the internet. We will also see how you can extract knowledge from PDF files.\n",
        "\n",
        "Additionally, using python helps us keep track of any changes made along the way, which is great for collaborative team work. Python also ensures that every time we run a task, it gives us consistent results, making it easier to check if something goes wrong and fix it. This reliability helps make our solutions stronger and more dependable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will be making use of the code library called `langchain` to call on the OpenAI models. It is a simple and relatively user-friendly way to interact with the model, making it a great choice for beginners or for those looking to get things done quickly. This method provides us access to a `temperature` parameter, which determines the randomness or stochastic nature of the output the model will give us. A temperature of 1 makes the model more random, while a temperature close to 0 makes the model more deterministic with the same output for repeated queries. Most people prefer the output to be somewhat creative in nature with a temperature value of around 0.7, which is the default value set for the OpenAI models. You can try changing this value and experience how the output changes for the same query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code snippets below we call on the API. Model is specified via the `model` parameter on the second row. A complete list of available models to choose from can be found [here](https://platform.openai.com/docs/models/overview). We will start by using one of the GPT-3.5 turbo models. \n",
        "\n",
        "* **gpt-3.5-turbo** uses the latest available model version of GPT-3.5. Allows for up to **4,097 tokens** as input and output\n",
        "    * **gpt-3.5-turbo-0613** is the latest version of GPT-3.5, currently the same as above\n",
        "    * **gpt-3.5-turbo-16k** is a version of GPT-3.5 with longer context window, up to **16,385 tokens**\n",
        "\n",
        "</b>\n",
        "\n",
        "* **gpt-4** uses the latest available model version of GPT-4. Allows using **8,192 tokens** as input and output\n",
        "    * **gpt-4-0613** is the latest version of GPT-4, same as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This creates an instance of the model interface which we can subsequently call on\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    # The below parameters can be changed\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB 1: The GPT-3.5 model usually takes between 15 to 45 seconds to respond. If it takes more than a minute, stop the execution and rerun the cell block.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB 2: This is one option for implementing the LLM call, which is nice due to its simple and intuitive format. At the bottom of this section we explore using the OpenAI API directly as an alternative option, which provides even greater control of the input/output.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `chat_model` instance accepts a list of `messages` as input. We specify messages according to a schema used by various functions in the `langchain` library. This will facilitate some of the more advanced functionalities we will make use of later. Messages in a chat conversation are generally provided as a `list` (inside square brackets) of conversation turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    # Overall system behaviour of the chat bot\n",
        "    SystemMessage,\n",
        "    # User message, query to chat bot\n",
        "    HumanMessage,\n",
        "    # The response from the chat bot, for conversational turns\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "\n",
        "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "\n",
        "# The user prompt is what would be written in the chat interface, your query\n",
        "user_prompt = \"What do you know about Patricia Industries from Sweden?\"\n",
        "\n",
        "\n",
        "# These are collected into a messages list of\n",
        "#\n",
        "#   SystemMessage - the system prompt\n",
        "#   HumanMessage  - the user query\n",
        "#   AIMessage     - the bot response, in case you wish to continue on a conversation\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_prompt),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant.'),\n",
              " HumanMessage(content='What do you know about Patricia Industries from Sweden?')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can check what the messages look like by printing it out\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see, the `messages` variable is a list containing our `SystemMessage` and `HumanMessage`.\n",
        "\n",
        "\n",
        "Once we have a list of messages in the above format it is easy to call the OpenAI model and get a response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patricia Industries is a Swedish investment company and a subsidiary of Investor AB, one of Sweden's largest industrial holding companies. Patricia Industries primarily focuses on long-term investments in companies within the industrial sector. They aim to be an active owner and partner to their portfolio companies, providing strategic guidance and support.\n",
            "\n",
            "Some of the key industries that Patricia Industries is involved in include healthcare, industrials, and consumer goods. They have made investments in various companies across these sectors, both in Sweden and internationally.\n",
            "\n",
            "Patricia Industries follows a long-term investment approach, aiming to create sustainable value in their portfolio companies. They prioritize companies with strong market positions, solid growth potential, and a commitment to responsible business practices.\n",
            "\n",
            "Overall, Patricia Industries plays an important role in the Swedish investment landscape, supporting the growth and development of companies in various sectors through their strategic investments and active ownership.\n"
          ]
        }
      ],
      "source": [
        "# Here we collect the output from the chat model in the variable response\n",
        "response = chat_model(messages)\n",
        "\n",
        "# We can print out the response by calling on its content using a .content\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The full response is actually formatted as a `AIMessage` as we can see by printing out the full response without using the print function. Observe that if we do not use the formatting of the `print` function you will see linebreak characters such as `\\n` appearing in the message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"Patricia Industries is a Swedish investment company and a subsidiary of Investor AB, one of Sweden's largest industrial holding companies. Patricia Industries primarily focuses on long-term investments in companies within the industrial sector. They aim to be an active owner and partner to their portfolio companies, providing strategic guidance and support.\\n\\nSome of the key industries that Patricia Industries is involved in include healthcare, industrials, and consumer goods. They have made investments in various companies across these sectors, both in Sweden and internationally.\\n\\nPatricia Industries follows a long-term investment approach, aiming to create sustainable value in their portfolio companies. They prioritize companies with strong market positions, solid growth potential, and a commitment to responsible business practices.\\n\\nOverall, Patricia Industries plays an important role in the Swedish investment landscape, supporting the growth and development of companies in various sectors through their strategic investments and active ownership.\")"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Giving ChatGPT a short-term memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can incorporate the `AIMessage` response in a new series of messages and ask a follow up question if you wish. This provides the illusion of the chat model having a memory and being able to continue a conversation for a few turns, as you have seen while working in the browser environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant.'),\n",
              " HumanMessage(content='What do you know about Patricia Industries from Sweden?'),\n",
              " AIMessage(content=\"Patricia Industries is a Swedish investment company and a subsidiary of Investor AB, one of Sweden's largest industrial holding companies. Patricia Industries primarily focuses on long-term investments in companies within the industrial sector. They aim to be an active owner and partner to their portfolio companies, providing strategic guidance and support.\\n\\nSome of the key industries that Patricia Industries is involved in include healthcare, industrials, and consumer goods. They have made investments in various companies across these sectors, both in Sweden and internationally.\\n\\nPatricia Industries follows a long-term investment approach, aiming to create sustainable value in their portfolio companies. They prioritize companies with strong market positions, solid growth potential, and a commitment to responsible business practices.\\n\\nOverall, Patricia Industries plays an important role in the Swedish investment landscape, supporting the growth and development of companies in various sectors through their strategic investments and active ownership.\"),\n",
              " HumanMessage(content='Can you say something more about the investment approach?')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's ask a follow-up question which refers to the previous conversation without explicitly mentioning e.g., Patricia Industries\n",
        "follow_up_question = \"Can you say something more about the investment approach?\"\n",
        "\n",
        "# We can now add the response we got from the first query together with our follow-up question\n",
        "# In python, you can add lists together to form a new list which makes it easy to add new conversation turns into the messages variable\n",
        "# Not that we add both the chat bot response and our follow up question\n",
        "messages = messages + [response, HumanMessage(content=follow_up_question)]\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Certainly! Patricia Industries follows a patient and long-term investment approach. They aim to be a stable and supportive partner to their portfolio companies, staying invested for the long haul and providing strategic guidance throughout the journey.\n",
            "\n",
            "When it comes to making investments, Patricia Industries looks for companies with strong market positions, potential for sustainable growth, and a clear strategic direction. They prioritize businesses that have demonstrated resilience, innovation, and the ability to adapt to changing market dynamics.\n",
            "\n",
            "Once an investment is made, Patricia Industries takes an active ownership role. They work closely with the management teams of their portfolio companies, providing strategic advice and operational support to drive growth and enhance value creation. This can involve assisting with strategic planning, business development, operational improvements, and capital allocation decisions.\n",
            "\n",
            "Furthermore, Patricia Industries emphasizes responsible business practices in their investments. They prioritize companies that prioritize sustainability, good corporate governance, and ethical behavior. They actively work with their portfolio companies to promote and enhance their sustainability efforts.\n",
            "\n",
            "Overall, Patricia Industries' investment approach combines a long-term perspective, active ownership, strategic guidance, and a focus on responsible business practices to create sustainable value in their portfolio companies.\n"
          ]
        }
      ],
      "source": [
        "# Here we collect the ouput from the chat model in the variable response\n",
        "response = chat_model(messages)\n",
        "\n",
        "# We can print out the response by calling on its content using a .content\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By continuing in this fashion we can record a conversation history which we send to the chat model as long as the total text content does not exceed the models content limit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OPTIONAL: Using the OpenAI API directly instead of the `langchain` wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below code snippet calls the OpenAI API directly, which allows us to access **all** of the available input and output options. While this approach offers a higher degree of control, it may not be as easy to use or straightforward to interpret as the functions from langchain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patricia Industries is a Swedish investment company that is part of the Investor AB group. It was founded in 2006 and focuses on long-term investments in companies with strong growth potential. Patricia Industries aims to actively develop its portfolio companies and support their strategic development. The company has investments in various industries such as healthcare, technology, and consumer goods. Some notable investments of Patricia Industries include Mölnlycke Health Care, Permobil, and Aleris.\n"
          ]
        }
      ],
      "source": [
        "# Needs to be set for the OpenAI API to be callable\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "# Model and parameters (almost all that are available, logit_bias not included). Parameters model and messages are required\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "max_tokens = 1024           # Max nr of tokens to generate in the chat completion, limited by model choice\n",
        "temperature = 0.7           # Value in (0, 2). Sampling temperature for stochastic nature in response\n",
        "top_p = 1                   # Vale in (0, 1). Nucleus sampling, optional to temperature, considers tokens comprising top_p probability mass\n",
        "frequency_penalty = 0       # Value in (-2, 2). Positive value penalizes new tokens based on frequency in text so far, to decrease likelihood of repeating sentences\n",
        "presence_penalty = 0        # Value in (-2, 2). Positive value penalizes new tokens if appeared in text so far, to increase likelihood of switching to new topics\n",
        "n=1                         # How many completion options to generate for each message\n",
        "stream = False              # Boolean value which can allow for streaming response\n",
        "\n",
        "# More advanced options which allow to include function calling capability for the model itself. We will not use this functionality in the exercises below\n",
        "function_call = \"none\"      # 'auto' if call function or generate message, supply {\"name\": my_func}\n",
        "functions = [{\"name\": \"Name\", \"description\": \"semantic description of what the function(s) do\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}]\n",
        "\n",
        "\n",
        "# Messages are provided as a list, similarly to the langchain method, but formatted differently\n",
        "# Other possible values for the key 'role' are 'assistant' and 'function'\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "user_prompt = \"What do you know about Patricia Industries from Sweden?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "\n",
        "# API call, collected in the variable response\n",
        "response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=top_p,\n",
        "            frequency_penalty=frequency_penalty,\n",
        "            presence_penalty=presence_penalty,\n",
        "            n=n,\n",
        "            stream=stream,\n",
        "            function_call=function_call,\n",
        "            functions=functions\n",
        "        )\n",
        "\n",
        "\n",
        "# Printing the response requires a slighty less intuitive format (see below for full unformatted output)\n",
        "print(response.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can take a look at the complete response without simplifying or changing the output in any way. When you print the information from the code below, you will see that the response includes details about token usage and other aspects. These details are important as they help you estimate and collect the cost of running queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-8GhT5P9wabfFTvWGFPHDXw8uzV4Ff at 0x12e120710> JSON: {\n",
              "  \"id\": \"chatcmpl-8GhT5P9wabfFTvWGFPHDXw8uzV4Ff\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1698991031,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"Patricia Industries is a Swedish investment company that is part of the Investor AB group. It was founded in 2006 and focuses on long-term investments in companies with strong growth potential. Patricia Industries aims to actively develop its portfolio companies and support their strategic development. The company has investments in various industries such as healthcare, technology, and consumer goods. Some notable investments of Patricia Industries include M\\u00f6lnlycke Health Care, Permobil, and Aleris.\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 58,\n",
              "    \"completion_tokens\": 91,\n",
              "    \"total_tokens\": 149\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Printing the response variable without formatting via the print() function displays the full output from the model\n",
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connecting Language Models to the Internet: A way to Expand the models Knowledge base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will explore two simple open source options for adding internet content as context for our query to the LLM:\n",
        "\n",
        "* The Wikipedia API which allow us to read in relevant Wikipedia information to get factual information on a specific topic\n",
        "* The DuckDuckGo API which allows us to search updated information on a topic, to use as context for our query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wikipedia API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we wish to look up or summarize information from Wikipedia and we know the title of the page we're interested in, we can easily use the Wikipedia API to grab the content from that page. However, it's important to know that the Wikipedia API isn't designed for broad searches like a regular search engine. It works best when the search term you enter matches a Wikipedia page title quite closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Patricia_Arquette\n",
            "Patricia Arquette (born April 8, 1968) is an American actress. She made her feature film debut as Kristen Parker in A Nightmare on Elm Street 3: Dream Warriors (1987) and has since starred in several film and television productions. She has received several awards including an Academy Award, two Primetime Emmy Awards and three Golden Globe Awards.\n",
            "She had starring roles in several critically acclaimed films, including True Romance (1993), Ed Wood (1994), Flirting with Disaster (1996), Lost Highway (1997), The Hi-Lo Country (1998), and Bringing Out the Dead (1999). From 2005 to 2011, she starred as a character based on the medium Allison DuBois in the supernatural drama series Medium, winning the Primetime Emmy Award for Outstanding Lead Actress in a Drama Series in 2005.\n",
            "For playing a single mother in the coming-of-age film Boyhood (2014), which was filmed from 2002 until 2014, Arquette won the Academy Award for Best Supporting Actress. Further success came for starring as a prison wor\n"
          ]
        }
      ],
      "source": [
        "# Let's try it out by asking a general question (note that this is not the recommended way to interact with the Wikipedia API)\n",
        "query = \"What is Patricia Industries\"\n",
        "\n",
        "# Try searching with a general question.\n",
        "# Here, we are calling a function named wiki_search_api with two arguments; query and how_many\n",
        "# The how_many argument specifies how many letters/symbols we wish to fetch\n",
        "result_wiki = wiki_search_api(query, how_many=1000)\n",
        "\n",
        "# We print the result to the console. This will display the first 1000 characters of the Wikipedia page related to our query\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see the answer is not related to Patricia Industries at all. Instead the response is from a popular Wikipedia page with a Title trying to match our query.\n",
        "\n",
        "\n",
        "By searching instead for a keyword we can increase the chances of obtaining a more relevant answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Investor_AB\n",
            "Investor AB is a Swedish investment and holding company, often considered a de facto conglomerate. One of Sweden's largest companies, Investor AB serves as the investment arm of the prominent Swedish Wallenberg family; the family's companies are involved in a variety of industries, of which the primary industries are pharmaceuticals, telecommunications and industry.Investor AB is Sweden's most valuable publicly traded company; it has major or controlling holdings in several of Sweden's other largest companies. It has major investments worldwide through Patricia Industries and EQT Partners.\n",
            "\n",
            "\n",
            "== History ==\n",
            "Investor AB was established in Stockholm when new Swedish legislation made it more difficult for banks to own stocks in industrial companies on a long-term basis. The shareholdings of the Wallenberg family bank, Stockholms Enskilda Bank, were transferred to Investor AB, a newly formed industrial holding company spun off from the bank, which would serve as the family's investment arm h\n"
          ]
        }
      ],
      "source": [
        "# This is the keyword you want to search for on Wikipedia.\n",
        "query_keyword = \"Patricia Industries\"\n",
        "\n",
        "# By searching on the keyword we obtain a better match than if asking a general question. \n",
        "result_wiki = wiki_search_api(query_keyword, how_many=1000)\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can customize this somewhat by adding a function which extracts the keyword from a sentence by looking for the phrase `keyword: <KEYWORD>`. This has some limitiations though since it requires us to include this kind of formatted phrase in our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Investor_AB\n",
            "Investor AB is a Swedish investment and holding company, often considered a de facto conglomerate. One of Sweden's largest companies, Investor AB serves as the investment arm of the prominent Swedish Wallenberg family; the family's companies are involved in a variety of industries, of which the primary industries are pharmaceuticals, telecommunications and industry.Investor AB is Sweden's most valuable publicly traded company; it has major or controlling holdings in several of Sweden's other largest companies. It has major investments worldwide through Patricia Industries and EQT Partners.\n",
            "\n",
            "\n",
            "== History ==\n",
            "Investor AB was established in Stockholm when new Swedish legislation made it more difficult for banks to own stocks in industrial companies on a long-term basis. The shareholdings of the Wallenberg family bank, Stockholms Enskilda Bank, were transferred to Investor AB, a newly formed industrial holding company spun off from the bank, which would serve as the family's investment arm h\n"
          ]
        }
      ],
      "source": [
        "query_keyword = \"Patricia Industries\"\n",
        "query_with_keyword = f\"What is keyword: {query_keyword}\"\n",
        "\n",
        "# By searching on the keyword we obtain a better match. \n",
        "# The find_keyword() function below extracts the keyword, so this is doing the same as we did above by supplying the keyword directly\n",
        "result_wiki = wiki_search_api(find_keyword(query_with_keyword), how_many=1000)\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own keyword. The more closely this matches an existing Wikipedia title page, the more likely you are of finding what you look for."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicitly using the query for the purpose of answering the question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Ellipsis\n",
            "The ellipsis ... (; also known informally as dot dot dot) is a series of dots that indicates an intentional omission of a word, sentence, or whole section from a text without altering its original meaning. The plural is ellipses. The term originates from the Ancient Greek: ἔλλειψις, élleipsis meaning 'leave out'.Opinions differ as to how to render ellipses in printed material. According to The Chicago Manual of Style, it should consist of three periods, each separated from its neighbor by a non-breaking space: . . .. According to the AP Stylebook, the periods should be rendered with no space between them: .... A third option is to use the Unicode character U+2026 … HORIZONTAL ELLIPSIS.\n",
            "\n",
            "\n",
            "== Background ==\n",
            "The ellipsis is also called a suspension point, points of ellipsis, periods of ellipsis, or (colloquially) \"dot-dot-dot\". Depending on their context and placement in a sentence, ellipses can indicate an unfinished thought, a leading statement, a slight pause, an echoing voice, or a ner\n"
          ]
        }
      ],
      "source": [
        "# Step 1: \n",
        "# Choose a keyword for your search. It's better to pick a keyword that closely matches an existing Wikipedia page title.\n",
        "# This increases your chances of finding the information you're looking for. \n",
        "# If you can't come up with something try query_keyword = \"rubberducking\"\n",
        "query_keyword = \"...\"\n",
        "\n",
        "# Step 2: \n",
        "# Now, let's search Wikipedia using the keyword you chose. \n",
        "# The 'how_many' argument tells the program how many characters of the result you want to retrieve.\n",
        "# In this case, we've set it to 1000, so you'll see the first 1000 characters of the Wikipedia page.\n",
        "result_wiki = wiki_search_api(query_keyword, how_many=1000)\n",
        "\n",
        "# Step 3: \n",
        "# Print the result to the screen so that you can read it.\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicitly using Wikipedia facts for the purpose of answering a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: \n",
        "# Setting up the Model\n",
        "# We're preparing to use a specific AI model by setting some initial parameters.\n",
        "# One of these parameters is 'temperature', which affects how creative or strict the model's responses will be.\n",
        "# A lower temperature like 0.0 makes the model more focused and less random in its responses.\n",
        "temperature = 0.0\n",
        "\n",
        "\n",
        "# Now we create an instance of the ChatOpenAI model, which we'll use to generate responses.\n",
        "# We're specifying which model to use ('gpt-3.5-turbo') and providing our OpenAI API key to authorize access.\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: \n",
        "# Defining a Prompt Function to Use the Model\n",
        "# We're creating a function to make it easier to use the model with different questions and keywords.\n",
        "# This function will search Wikipedia for context, then ask the model to answer a query based on that context.\n",
        "def LLM_with_wiki_prompt(query: str, query_keyword: str, how_many: int=2000, temperature: float=0.0):\n",
        "    '''\n",
        "    Prompt function for calling the chat model with a general query, where we believe relevant information can\n",
        "    be found on a Wikipedia page whose title matches the query_keyword. \n",
        "    We an optionally change how many characters should be retrieved and what the temperature of the model should be.\n",
        "    '''\n",
        "\n",
        "    # Retrieve serch results based the query\n",
        "    # This line searches Wikipedia using the keyword provided, and gets up to 2000 characters of text as context.\n",
        "    context = wiki_search_api(query_keyword, how_many=how_many)\n",
        "\n",
        "    # System prompt\n",
        "    # We're setting up a system prompt to instruct the model on how to approach answering the query.\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful investment management expert.\n",
        "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
        "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
        "    Then proceed to answer in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt inputs the query and telling the model to consider the Wikipedia context we found.\n",
        "    user_prompt = f\"\"\"\n",
        "    Consider carefully the following wiki facts as context.\n",
        "    context: {context}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following query and summarize your response.\n",
        "    query: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context,\n",
        "    then return those bullets verbatim and do not try to rephrase them.\n",
        "    \"\"\"\n",
        "\n",
        "    # We're arranging the system and user prompts into a list of messages to send to the model.\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "\n",
        "\n",
        "    # We ask the model to generate a response based on the messages, and print that response.\n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Investor_AB\n",
            "- Investor AB has major investments worldwide through Patricia Industries.\n",
            "- Patricia Industries is a subsidiary or division of Investor AB.\n",
            "- Patricia Industries serves as the investment arm of Investor AB for its global investments.\n",
            "- Patricia Industries is involved in various industries, including pharmaceuticals, telecommunications, and industry, similar to Investor AB's primary industries.\n"
          ]
        }
      ],
      "source": [
        "# We can now ask a general question as our query\n",
        "query = \"What is the relationship between Patricia Industries and Investor AB?\"\n",
        "# Together with a keyword argument which should match a Wikipedia title where relevant information can be found\n",
        "query_keyword = \"Patricia Industries\"\n",
        "\n",
        "# Using the above query and keyword as arguments, we call the prompt function\n",
        "LLM_with_wiki_prompt(query=query, query_keyword=query_keyword, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query and relevant keyword. The more closely the keyword matches an existing Wikipedia title page, the more likely it is that the LLM will be able to answer your query based on relevant Wikipedia facts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Ellipsis\n",
            "The ellipsis, also known as dot dot dot, is a series of dots that indicates an intentional omission of a word, sentence, or whole section from a text without altering its original meaning. It can be rendered as three periods with a non-breaking space between them ( . . . ) according to The Chicago Manual of Style, or with no space between them (....) according to the AP Stylebook. Another option is to use the Unicode character U+2026 (… HORIZONTAL ELLIPSIS).\n",
            "\n",
            "Ellipses can indicate an unfinished thought, a leading statement, a slight pause, an echoing voice, or a nervous or awkward silence. They can be used to trail off into silence or suggest melancholy or longing when placed at the end of a sentence.\n",
            "\n",
            "The most common forms of an ellipsis are a row of three periods or the horizontal ellipsis glyph. Style guides have their own rules for the use of ellipses, such as using three periods with spaces on both sides or putting the dots together with a space before and after the group.\n",
            "\n",
            "There is a debate about whether an ellipsis at the end of a sentence needs a fourth dot to finish the sentence. Some style guides, like Chicago and APA, advise using a fourth dot, while others do not.\n"
          ]
        }
      ],
      "source": [
        "# Assign the query string, which is the question you want to answer.\n",
        "query = \"...\"\n",
        "\n",
        "# Assign the query \"keyword\" string, which is the keyword you want to use to fetch relevant context from Wikipedia.\n",
        "query_keyword = \"...\"\n",
        "\n",
        "\n",
        "# Now, you are calling the function LLM_with_wiki_prompt which was defined earlier.\n",
        "# You are passing the query, query_keyword, and temperature (defined earlier) as arguments to the function.\n",
        "# This function is expected to search Wikipedia using the query_keyword, \n",
        "# then use the model to answer the query based on the Wikipedia information,\n",
        "# and finally the function prints out the model's response.\n",
        "LLM_with_wiki_prompt(query=query, query_keyword=query_keyword, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DuckDuckGo search API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [DuckDuckGo browser](https://duckduckgo.com/) provides an open source API which can be used to browse and search the internet. This returns a search response to a query and can be useful for incorporating a small amount of updated information as context to your LLM queries. More advanced search functionalities are possible but quickly become more technically involved for the user, so for the purpose of this short course we restrict to the basic search functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Get Investor AB (INVEb.ST) real-time stock quotes, news, price and financial information from Reuters to inform your trading and investments Latest news about Investor AB (publ) Investor AB(OM:INVE A) dropped from S&P Global BMI Index ... Investor AB is a Sweden-based industrial holding company. Its operations are divided into three business segments: Listed Core Investments, EQT and Patricia Industries. The Listed Core Investments segment consists of listed holdings, which embrace ... Investor AB : News, information and stories for Investor AB | Nasdaq Stockholm: INVE B | Nasdaq Stockholm ... Business Leaders. Sectors. All our articles. Most Read News. ... CEO Johan Forssell will leave Investor in May 2024 in a new role with focus on Oct. 20. AQ Johan Forssell to Leave from Investor AB as President and Director, Effective 7 ... Latest news about Investor AB (publ) Markets Equities News Today Investor AB publishes the Annual Report 2022 including the Sustainability Report, on www.investorab.com. While 2022 was challenging for the global economy, Investor and our portfolio showed... 920 followers $18.97 0.00 ( 0.00%) 3:06 PM 10/06/23 Pink Limited Info | $USD | Delayed Summary Ratings Financials Earnings Dividends Valuation Growth Profitability Momentum Peers Options Charting...\n"
          ]
        }
      ],
      "source": [
        "# Let's pose a query for which we wish to retrieve information from the internet\n",
        "query = \"What's the latest business news about Investor AB?\"\n",
        "\n",
        "# Note that the retrieved responses are not necessarily the same every time we run a search.\n",
        "# The max_results argument is a cap of the nr of retrieved search results\n",
        "response = get_search_results_ddg(query, max_results=50)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that you receive a collection of relevant snippets to your query and not an actual answer. These snippets correspond to small amount of information retrieved from various web pages based om the query. To collect the retrieved information into a more summarized answer we can use the LLLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicitly using the query for the purpose of answering a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to implement the internet search functionality together with the LLM for answering our queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: \n",
        "# Setting up the Model\n",
        "# We're preparing to use a specific AI model by setting some initial parameters.\n",
        "# One of these parameters is 'temperature', which affects how creative or strict the model's responses will be.\n",
        "# A lower temperature like 0.0 makes the model more focused and less random in its responses.\n",
        "temperature=0.0\n",
        "\n",
        "\n",
        "# Now we create an instance of the ChatOpenAI model, which we'll use to generate responses.\n",
        "# We're specifying which model to use ('gpt-3.5-turbo') and providing our OpenAI API key to authorize access.\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model=\"gpt-3.5-turbo\", # Specifying the model version to be used.\n",
        "    temperature=temperature # Setting the temperature parameter for the model.\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: \n",
        "# Defining a Prompt Function to Use the Model\n",
        "# We define a function for more easily calling the LLM using the same prompt template but with a different query\n",
        "# The function uses DuckDuckGo to retrieve context based on the query and then use that to get a response from the LLM\n",
        "def LLM_with_search_prompt(query: str, max_results: int=10, temperature: float=0.0):\n",
        "\n",
        "    # Retrieving search results based on the query provided, limited to max_results.\n",
        "    context = get_search_results_ddg(query, max_results=max_results)\n",
        "\n",
        "     # Defining the System Prompt to instruct the model.\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful investment management expert.\n",
        "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
        "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
        "    Then proceed to answer in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt inputs the query and context\n",
        "    user_prompt = f\"\"\"\n",
        "    One of our portfolio companies have handed us these news reports as context.\n",
        "    context: {context}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following query and summarize your response.\n",
        "    query: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context,\n",
        "    then return those bullets verbatim and do not try to rephrase them.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Creating a list of message objects for interaction with the model.\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "\n",
        "    # Calling on the chat_model with the constructed messages and specified temperature, then printing the model's response.\n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the above function will print out the response. Note that running the below cell may result in different outputs despite setting the temperature parameter to 0. This is because the retrieved answers from the DuckDuckGo search are not always the same, which means that the LLM may consider different contexts when trying to answer your query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, there is no specific mention of the latest business news about Investor AB and its board. The context mainly provides information about Investor AB's operations, business segments, and listed holdings. Therefore, there are no bulleted points or specific news about Investor AB and its board to summarize.\n"
          ]
        }
      ],
      "source": [
        "# Let's pose a query for which we wish to retrieve information from internet and get an LLM response based on that information\n",
        "query = \"What's the latest business news about Investor AB and its board?\"\n",
        "\n",
        "# Here we use our prompt function to call the LLM\n",
        "LLM_with_search_prompt(query=query, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query. The context will be automatically included via the prompt function defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Rubberducking is a debugging technique commonly used in programming and software development.\n",
            "- It involves explaining your code and the problem you are trying to solve to a rubber duck or any inanimate object.\n",
            "- The act of speaking out loud helps in problem-solving and can lead to finding solutions to coding issues.\n",
            "- Many programmers keep a rubber duck on their desk for debugging emergencies.\n",
            "- Rubber duck debugging is a step-by-step process that starts with getting your rubber duck and explaining your goal or what you are trying to achieve with your code.\n"
          ]
        }
      ],
      "source": [
        "# Write your own query and retrieve search results from that as context for the LLM\n",
        "query = \"What is rubberducking?\"\n",
        "\n",
        "LLM_with_search_prompt(query=query, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF knowledge base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by providing the location and names of the PDF files which we will be exploring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "N8E7XBwp_rho"
      },
      "outputs": [],
      "source": [
        "# Absolute or relative path to folder where the PDF files are located\n",
        "#DATAPATH = \"../Data/\"\n",
        "DATAPATH = \"/content/drive/MyDrive/GenAI/GenAI short course/\"\n",
        "\n",
        "\n",
        "\n",
        "# Name of the PDF files (without the trailing .pdf file endings)\n",
        "Patricia_permobil_manual = \"Patricia_permobil_manual\"\n",
        "caterpillar_10k = \"Caterpillar-10k\"\n",
        "whirlpool_10k = \"whirlpool-10k\"\n",
        "electrolux_ann_rep = \"Electrolux-annual-report\"\n",
        "\n",
        "\n",
        "# We collect all the PDF file names in a list\n",
        "pdf_files = [\n",
        "    Patricia_permobil_manual,\n",
        "    caterpillar_10k,\n",
        "    whirlpool_10k,\n",
        "    electrolux_ann_rep\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **We load and inspect the text converted PDF files and some properties below**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now load one of the PDFs and examine its content. Note that this relies on extracting the content of the PDF as text and works well for the parts which are written text in the PDF, but works less well for e.g., tables and figures. In order to handle tables and figures well we typically need to perform pre-processing steps which are more complex and tailored for the specific PDF file type we wish to examine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[103m\u001b[1mNr of pages in pdf document:\u001b[0m 96 \n",
            "\n",
            "The document contains \u001b[31m\u001b[103m\u001b[1m195303\u001b[0m symbols/characters\n",
            "\u001b[31m\u001b[103m\u001b[1m\n",
            "\n",
            "Ex text from first page:\u001b[0m\n",
            "\n",
            "ZR Owner’s Manual\n",
            "OM0005_Rev A_ZR\n",
            "ii\n",
            "REGISTER YOUR TiLITE\n",
            "Register online at TiLite.com\n",
            "or\n",
            "Complete and mail the form on the next page\n",
            "Why Should You Register:\n",
            "1. Increase your use and enjoyment of your TiLite by receiving updates from\n",
            "    TiLite with product information, maintenance tips and industry news.\n",
            "2. Enable TiLite to contact you or your health care provider if servicing is\n",
            "    needed for your wheelchair.\n",
            "3. Provide your feedback to TiLite regarding your experience and needs,\n",
            "    thereby enabling TiLite to further improve product designs.\n",
            "All information you provide to TiLite when you register will be protected by TiLite as \n",
            "required by applicable laws and regulations and will be used solely by TiLite.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pdf_file = Patricia_permobil_manual\n",
        "\n",
        "# Here we read in the PDF as text using the function text_from_pdf(), defined in the file helper_functions.py\n",
        "pdf_text = text_from_pdf(DATAPATH + pdf_file + \".pdf\")\n",
        "\n",
        "\n",
        "# Here we count and print out the total nr of pages in the retrieved PDF\n",
        "print(f\"{Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}Nr of pages in pdf document:{Style.RESET_ALL} {len(pdf_text.keys())} \\n\")\n",
        "\n",
        "\n",
        "# Here we count and print out the total nr of symbols in the retrieved PDF file\n",
        "concat_text = pdf_dict_to_str(text_from_pdf(DATAPATH + pdf_file + \".pdf\"))\n",
        "print(f\"The document contains {Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}{len(concat_text)}{Style.RESET_ALL} symbols/characters\")\n",
        "\n",
        "\n",
        "# We then print an example page from the PDF\n",
        "print(f\"{Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}\\n\\nEx text from first page:{Style.RESET_ALL}\\n\")\n",
        "print(f\"{pdf_text['page_3']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OPTIONAL: The following section performs chunking of text and creates indexed embeddings for the chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The chat models have a limit in the number of tokens/characters they can receive as input. This means we cannot feed them the full PDF as context. Rather we need to find the most relevant pieces of text within the PDF and use that when asking the LLM to respond to a query about the PDF.\n",
        "\n",
        "Here we split up the four PDF documents into piecewise chunks of text for embedding purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OSpFUPzL_tqN"
      },
      "outputs": [],
      "source": [
        "chunked_pdf_files_dict = {}  # Initialize an empty dictionary to store the chunked pdf data.\n",
        "for pdf_file in pdf_files:  # Iterate through each pdf file in the provided list.\n",
        "    pdf_location = DATAPATH + pdf_file + \".pdf\"  # This is the file path for the current pdf file.\n",
        "   \n",
        "    # Call the chunk_documents function to split the current pdf document into smaller text chunks.\n",
        "    chunked_pdf = chunk_documents(\n",
        "        documents= TextLoader(file_path=pdf_location, loader=PyPDFLoader),  # Loads the text content of the current pdf file.\n",
        "        TextSplitter=RecursiveCharacterTextSplitter,  # Specify the text splitting mechanism to be used.\n",
        "        chunk_size=512, # 512 is the maximum Sequence length  \n",
        "        chunk_overlap=20,  # Define the number of overlapping characters between adjacent chunks.\n",
        "        separator=None,  # No specific separator is used between chunks.\n",
        "        )\n",
        "        \n",
        "    # Store the chunked text data of the current pdf file in the dictionary using the file name as the key.    \n",
        "    chunked_pdf_files_dict[pdf_file] = chunked_pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's print out a sample chunk so that we can see what kind of information is available within a single chunk of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. The assistant at the rear of the chair is in control of this procedure. He or she must tilt the chair back to its balance point on the rear wheels. NEVER attempt to lift a wheelchair by lifting on any removable (detachable) parts, including upholstery and removable push handles or push handle grips. \n",
            "4. The second assistant at the front must firmly grasp a non-detachable part of the front frame (but NOT swing away hangers) with both hands and lift the chair up and over one stair at a time.\n"
          ]
        }
      ],
      "source": [
        "print(chunked_pdf_files_dict[Patricia_permobil_manual][79].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also inspect a chunk which contains a table, to see what it looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agreement.  At select business units, we have hired certain highly specialized employees under employment contracts that specify a\n",
            "term of employment, pay and other benefits.\n",
            " \n",
            "Full-T ime Employees at Year-End\n",
            " 2022 2021\n",
            "Inside U.S. 48,200 44,300\n",
            "Outside U.S. 60,900 63,400\n",
            "Total 109,100 107,700\n",
            "By Region:   \n",
            "North America 48,700 44,700\n",
            "EAME 16,900 17,600\n",
            "Latin America 19,100 19,500\n",
            "Asia/Pacific 24,400 25,900\n",
            "Total 109,100 107,700\n"
          ]
        }
      ],
      "source": [
        "# The below chunk contains a table from the caterpillar_10k PDF.\n",
        "print(chunked_pdf_files_dict[caterpillar_10k][104].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Performing the embedding of the chunked PDF documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_pdfs = False\n",
        "save_embeddings = False\n",
        "\n",
        "# For embeddings we use the nr 1 model on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
        "embedding_model = [\"BAAI/bge-large-en-v1.5\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: The below code block takes around 100 minutes to complete for the four PDF's (100-150 pages each), so this has been prepared before the lecture. We have kept the code here for those interested, but `DO NOT RUN THE BELOW TWO CELLS!` or `DO NOT MODIFY THE embed_pdfs = False and save_embeddings = False!`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "if embed_pdfs:\n",
        "\n",
        "  embedding_dict = {}\n",
        "  for model_name in embedding_model:\n",
        "    for pdf_file in pdf_files:\n",
        "      print(f\"Creating FAISS embedding for document {pdf_file} using - {model_name}\")\n",
        "      start = timer()\n",
        "      embedding = doc_embedding(embedding_model=model_name)\n",
        "      faiss_index = make_index_FAISS(chunked_documents=chunked_pdf_files_dict[pdf_file], embedding=embedding)\n",
        "      end = timer()\n",
        "      time_taken=end-start\n",
        "      print(f\"Done! Embedded vector index created after {time_taken/60:.2f} minutes\")\n",
        "      print(\"*\"*25)\n",
        "      embedding_dict[pdf_file] = faiss_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we save the embeddings so that we don't have to rerun the above every time. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "if save_embeddings:\n",
        "    for pdf_embedding in embedding_dict.keys():\n",
        "        FAISS_folder_name = DATAPATH + f\"FAISS_{pdf_embedding}.faiss\"\n",
        "        embedding_dict[pdf_embedding].save_local(FAISS_folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the above two code blocks have been run in advance in order to save the embeddings. This allows us to avoid doing this during the live session."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the saved embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If embeddings are already saved we can simply load them from storage. This avoids repeating the embedding process which can be quite time consuming. In a production environment you typically do this kind of indexing once, or on a running schedule if you expect the documents you embed will change over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We load and collect the saved embeddings in a python dictionary. This only takes ~15 seconds to load\n",
        "faiss_embeddings_dict = {}\n",
        "if not save_embeddings:\n",
        "    for pdf_file in pdf_files:\n",
        "        FAISS_folder_name = DATAPATH + f\"FAISS_{pdf_file}.faiss\"\n",
        "        embedding = doc_embedding(embedding_model=embedding_model[0])\n",
        "        faiss_embeddings_dict[pdf_file] = FAISS.load_local(FAISS_folder_name, embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try out making a simple query against the saved embeddings by doing a similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found on page: 78\n",
            "\n",
            "11-4\n",
            "ZR Owner’s Manual OM0005_Rev A_ZRCHAPTER 11: CASTERS AND FORKSNote: TiLite designs its rigid wheelchairs to be flexible for improved maneuverability and increased ride comfort.  \n",
            "However, this flexibility requires that your chair be set up properly.  The following procedure will enable you to set up  your TiLite rigid wheelchair so it will perform to its potential.\n",
            "1. Place the wheelchair on a smooth, level surface with the casters trailing rearward.\n"
          ]
        }
      ],
      "source": [
        "# We choose one of the PDF files and a relevant search query\n",
        "pdf_file = Patricia_permobil_manual   # Specify the PDF file to be searched.\n",
        "similarity_query = \"What is the procedure for setting up my TiLite rigid wheelchair?\"\n",
        "\n",
        "\n",
        "# We retrieve the most semantically similar chunks by doing a similarity search over the saved embeddings in our faiss_embeddings_dict\n",
        "# The argument k sets how many such chunks we wish to retrieve\n",
        "test = faiss_embeddings_dict[pdf_file].similarity_search(similarity_query, k=10)\n",
        "\n",
        "\n",
        "# We can now print out the most similar chunk. We print out both the chunk and the PDF page on which it is located.\n",
        "# The retrieved chunks are collected in order of similarity from 0 to k-1, \n",
        "# so chunk_nr = 0 correspond to the most similar chunk\n",
        "chunk_nr = 0\n",
        "print(f\"Found on page: {test[chunk_nr].metadata['page']}\\n\")\n",
        "print(test[chunk_nr].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Querying the Permobil Manual PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now ready to start asking questions of our PDF files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: \n",
        "# Setting up the Model\n",
        "# We're preparing to use a specific AI model by setting some initial parameters.\n",
        "# One of these parameters is 'temperature', which affects how creative or strict the model's responses will be.\n",
        "# A lower temperature like 0.0 makes the model more focused and less random in its responses.\n",
        "temperature = 0.0\n",
        "\n",
        "\n",
        "# Now we create an instance of the ChatOpenAI model, which we'll use to generate responses.\n",
        "# We're specifying which model to use ('gpt-3.5-turbo') and providing our OpenAI API key to authorize access.\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: \n",
        "# Defining a Prompt Function to Use the Model\n",
        "# We define a function for more easily calling the LLM using the same prompt template but with a different query and context\n",
        "def LLM_with_pdf_context(context: str, query: str, temperature: float=0.0):\n",
        "    \n",
        "    # System prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful investment management expert.\n",
        "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
        "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
        "    Then proceed to answer in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt template inputs the merged context and query for the LLM\n",
        "    user_prompt = f\"\"\"\n",
        "    I need help from an investement management expert.\n",
        "    One of our portfolio companies have handed us these brief instruction snippets as\n",
        "    context: {context}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following \n",
        "    question: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context,\n",
        "    then return those instructions verbatim and do not try to rephrase them.\n",
        "    If you cannot answer based on the information in context, then do not try to answer, but instead must answer verbatim with:\n",
        "    {{There is no available information related to your query in the context!'}}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "    \n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We choose one of the PDF files and a relevant search query. This query is the same as we asked during session 1 using the AskYourPDF plugin\n",
        "pdf_file = Patricia_permobil_manual\n",
        "query = \"What tools are needed for adjusting the Front Seat Height on Slipstream Single-Sided Forks. Also provide the steps for performing the adjustment.\"\n",
        "\n",
        "\n",
        "# We retrieve the most similar chunks related to above search query and collect these into a context string\n",
        "faiss_index = faiss_embeddings_dict[pdf_file]  # Access the FAISS index for the specified PDF file.\n",
        "top_hits = similarity_search_FAISS(search_query=query, nr_hits=5, index_store=faiss_index)  # Perform a similarity search in the specified PDF file for the given query, retrieving the top 5 similar chunks.\n",
        "context = \"\".join([document.page_content for document in top_hits])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's use all of the retrieved top hits as context when querying the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the information provided in the context, the tools needed for adjusting the Front Seat Height on Slipstream Single-Sided Forks are:\n",
            "\n",
            "- 5/8\" Open End Wrench\n",
            "- Screwdriver\n",
            "\n",
            "The steps for performing the adjustment are as follows:\n",
            "\n",
            "1. Remove the caster. Refer to \"Slipstream Single-Sided Forks - Replacing Casters\" on page 11-2 for instructions on how to remove the caster.\n",
            "2. Follow the procedures under \"Standard Forks - Replacing Casters\" on page 11-1 to mount the casters in the alternative axle holes in the fork. This will allow you to adjust the Front Seat Height without changing the casters to a larger or smaller size.\n",
            "3. Note that the full range of adjustability may not be available with 5\" or 6\" casters. Additional adjustability may be achieved with different forks or casters or with fork stem extensions.\n",
            "\n",
            "Please note that the instructions provided are verbatim from the context.\n"
          ]
        }
      ],
      "source": [
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the instructions are incomplete (these contain 7 steps in the PDF). This is due to the fact that the most relevant chunk happened to only contain the first part of the full instructions. Since the chunks are small this is something that can happen quite often. In order to get a more complete response we can try to feed not the retrieved and merged chunks as context, but instead feed the whole PDF page of the most relevant chunk, assuming it contains more fuller context. We can also do some combination of this and using other chunks if we find that top hit doesn't always work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the tools needed for adjusting the Front Seat Height on Slipstream Single-Sided Forks are:\n",
            "\n",
            "- 5/8\" Open End Wrench\n",
            "- Screwdriver\n",
            "\n",
            "The steps for performing the adjustment are as follows:\n",
            "\n",
            "1. Remove the caster. Refer to \"Slipstream Single-Sided Forks - Replacing Casters\" on page 11-2 for instructions.\n",
            "2. Using the shaft of the screwdriver, remove the E-Ring by pressing downward across the open portion of the E-Ring. Ensure you wear protective eyewear. See Figure 11-3.\n",
            "3. Using the 5/8\" Open End wrench, remove the axle from the Slipstream Single-Sided Fork.\n",
            "4. Place the axle in the alternate axle hole and securely tighten.\n",
            "5. Using the shaft of the screwdriver, replace the E-Ring by pressing downward across the closed portion of the E-Ring, snapping the E-ring into place. See Figure 11-3.\n",
            "6. Replace the caster. Refer to \"Slipstream Single-Sided Forks - Replacing Casters\" on page 11-2 for instructions.\n",
            "7. Follow Steps 1 through 6 on the opposite fork.\n",
            "\n",
            "Please note the warnings provided in the context regarding the use of identical axle holes and the need to reapply Vibra-TITE® VC-3 coating after every fourth adjustment.\n"
          ]
        }
      ],
      "source": [
        "# Here we get the page of the top hit and use that whole PDF page as context\n",
        "context_page = f\"\"\"{pdf_text[f'page_{top_hits[0].metadata[\"page\"]}']}\"\"\"\n",
        "\n",
        "# Use the pre-defined function but with the whole context page as context\n",
        "LLM_with_pdf_context(context_page, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case this worked much better, since the retrieved page actually contained the full instructions. In real production applications you may not always be so lucky, the instructions could for example be spread over multiple pages, so some work is often required to ensure consistency over a large corpus of texts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Querying the Electrolux Annual Report PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try asking some questions based on the content in the Electrolux's annual report for 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set our pdf variable to the Electrolux annual report\n",
        "pdf_file = electrolux_ann_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- January 11: Electrolux announced a loss for the fourth quarter of 2022, with an estimated operating income of approximately SEK -2.0bn (0.9), including non-recurring items of SEK -1.4bn (-0.7).\n",
            "- February 1: Electrolux decided to discontinue production at the Nyíregyháza factory in Hungary from the beginning of 2024.\n",
            "- September 9: Electrolux divested its business in Russia and sold its Russian subsidiary to local management, recording a capital loss of SEK 350m.\n",
            "- April 29 to September 2: Electrolux completed a share buyback program, repurchasing a total of 17,369,172 own series B shares for a total amount of SEK 3,032m.\n"
          ]
        }
      ],
      "source": [
        "query = \"Which major events happened for Electrolux during 2022?\"\n",
        "\n",
        "\n",
        "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
        "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "\n",
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query for information from the Electrolux Annual Report. You could, for example, ask questions about risks and challenges mentioned in the report or ask the model to explain Electrolux's business in Latin America."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the given context, it seems that there is a note repeated multiple times. The note consists of a sequence of numbers from 1 to 31, repeated multiple times.\n",
            "\n",
            "Without a specific question or query, it is difficult to provide a relevant answer. However, if the answer is provided in the form of a bulleted list within the context, we can return those instructions verbatim. Otherwise, if there is no relevant information in the context, we must answer with: \"There is no available information related to your query in the context!\"\n"
          ]
        }
      ],
      "source": [
        "# Write your own query and retrieve search results from that as context for the LLM\n",
        "query = \"...\"\n",
        "\n",
        "\n",
        "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
        "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "\n",
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Querying the Caterpillar 10-K PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try asking some questions based on the content in the Caterpillar 10-K annual report for 2022."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set our pdf variable to the Caterpillar 10-K report\n",
        "pdf_file = caterpillar_10k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The identified risks and challenges mentioned in the report are:\n",
            "\n",
            "- Delays and increased costs resulting from supply chain challenges and availability issues with suppliers.\n",
            "- Freight delays that could impact production in the company's facilities.\n",
            "- Fluctuations in freight costs, fuel costs, and limitations on shipping and receiving capacity.\n",
            "- Disruptions in the transportation and shipping infrastructure at important geographic points of exit and entry for the company's products.\n",
            "- Operating in different regions and countries exposes the company to numerous risks, including multiple and potentially conflicting laws, regulations, and policies that are subject to change.\n",
            "\n",
            "Please note that the information provided above is a direct excerpt from the context and is presented in a bulleted list format.\n"
          ]
        }
      ],
      "source": [
        "# Ask a relevant query\n",
        "query = \"Which identified risks and challenges are mentioned in the report?\"\n",
        "\n",
        "\n",
        "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
        "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "\n",
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tables in PDF-reports:**\n",
        "\n",
        "The Caterpillar 10-K report contains multiple tables which may cause a problem when using the text chunks for similarity retrieval. Here we can try and ask questions about information found in tables in the report. However, depending on the chunks, the model may confuse numbers found in the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, we can find the information about the number of full-time employees in Latin America at the end of the year 2022. \n",
            "\n",
            "The relevant information is mentioned in the snippet: \"Latin America\\nCurrent 770 400 150 69 26 20 — 1,435\". \n",
            "\n",
            "Therefore, the number of full-time employees in Latin America at the end of the year 2022 is 1,435.\n"
          ]
        }
      ],
      "source": [
        "# Let's ask a question which requires analyzing table information inside the PDF\n",
        "query = \"How many full-time employees did they have at the end of the year 2022 in Latin America?\"\n",
        "\n",
        "\n",
        "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
        "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "\n",
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query for information from the Caterpillar 10-K report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{There is no available information related to your query in the context!}\n"
          ]
        }
      ],
      "source": [
        "# Write your own query and retrieve search results from that as context for the LLM\n",
        "query = \"...\"\n",
        "\n",
        "\n",
        "# Use our function to retrieve the most similar chunks related to the search query for the given PDF\n",
        "context = get_contexts(pdf=pdf_file, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "\n",
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison between two PDF: Electrolux Annual Report and Whirlpool 10-K Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's go a bit deeper and try to compare information between two different PDF files. We can try to ask a question for which we expect there are answers in both the Electrolux annual report and the Whirlpool 10-K report. By using a query to retrieve relevant information from both of these reports we can use that as context to the LLM and ask about comparisons etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We define a function for more easily call the LLM using the same prompt template but with a different query and contexts\n",
        "def LLM_with_pdf_context_comparison(company1: str, company2: str, context1: str, context2: str, query: str, temperature: float=0.0):\n",
        "    \n",
        "    # System prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful investment management expert.\n",
        "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
        "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
        "    Then proceed to answer in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt inouts the merged context\n",
        "    user_prompt = f\"\"\"\n",
        "    Two of our portfolio companies have handed us these brief instruction snippets as\n",
        "    contexts. Contexts for {company1}: {context1}, and for {company2}: {context2}\n",
        "\n",
        "    Use only the above contexts and nothing else to answer the following \n",
        "    query: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context,\n",
        "    then return those instructions verbatim and do not try to rephrase them.\n",
        "    If you cannot answer based on the information in context, then do not try to answer, but instead must answer verbatim with:\n",
        "    {{There is no available information related to your query in the context!'}}\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "\n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the help of the above function we can pose queries which prompts the LLM to compare the information in different PDF's by creating separate contexts. Here, we compare the information in the Electrolux Annual report and Whirlpool 10-K report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's make a query which we would like to pose to both the Whirlpool and the Electrolux PDF files\n",
        "query = \"Which key business strategies were mentioned in the report?\"\n",
        "\n",
        "\n",
        "# Using the above query, we can fetch relevant contexts from both PDF files\n",
        "context_whirlpool = get_contexts(pdf=whirlpool_10k, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "context_electrolux = get_contexts(pdf=electrolux_ann_rep, query=query, faiss_embeddings_dict=faiss_embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given the above retrieved contexts we can now ask a comparative question between the companies\n",
        "chat_query = \"These are the key business strategies for Whirlpool and Electrolux. Compare them. Which strategies are similar and which are different?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- The key business strategies for Whirlpool include:\n",
            "  - Value creating approach enabled by three strong pillars: small appliances, major appliances in the Americas and India, and commercial appliances.\n",
            "  - Commitment to investing in businesses that support high growth and high margins.\n",
            "  - Growth strategies, financial results, product development, and sales efforts.\n",
            "  - Facing competition that may be able to quickly adapt to changing consumer preferences, particularly in the connected appliance space, or may be able to adapt more quickly to changes brought about by the global pandemic, supply chain constraints, inflationary pressures, currency fluctuations, geopolitical uncertainty.\n",
            "- The key business strategies for Electrolux include:\n",
            "  - Focus on driving sustainable consumer experience innovation and increasing efficiency.\n",
            "  - Long-term strategy based on five key industry trends.\n",
            "  - Financial targets for profitable growth over a business cycle.\n",
            "  - Clear strategy to deliver profitable growth and create shareholder value.\n",
            "\n",
            "Similar strategies:\n",
            "- Both Whirlpool and Electrolux focus on creating value and profitability.\n",
            "- Both companies emphasize innovation and product development.\n",
            "- Both companies consider competition and market trends in their strategies.\n",
            "\n",
            "Different strategies:\n",
            "- Whirlpool's strategy includes investing in businesses that support high growth and high margins, while Electrolux's strategy focuses on increasing efficiency and driving sustainable consumer experience innovation.\n",
            "- Whirlpool mentions specific pillars (small appliances, major appliances in the Americas and India, and commercial appliances), while Electrolux does not provide such specific pillars in their strategy.\n"
          ]
        }
      ],
      "source": [
        "# Use our pre-defined function together with the names of the companies and their context\n",
        "LLM_with_pdf_context_comparison(\"Whirlpool\", \"Electrolux\", context_whirlpool, context_electrolux, chat_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by telling your model what information from the reports you want to compare, for example, their sustainability strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There is no available information related to your query in the context!\n"
          ]
        }
      ],
      "source": [
        "# Write your own query on what you like to pose to both the Whirlpool and the Electrolux PDF files\n",
        "query = \"...\"\n",
        "\n",
        "\n",
        "# Using the above query, we can fetch relevant contexts from both PDF files\n",
        "context_whirlpool = get_contexts(pdf=whirlpool_10k, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "context_electrolux = get_contexts(pdf=electrolux_ann_rep, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "\n",
        "# Change this query to ask the model to compare ... for the companies.\n",
        "chat_query = \"This is the ... for Whirlpool and Electrolux. Compare them.\"\n",
        "\n",
        "\n",
        "# Use our pre-defined function together with the names of the companies and their context\n",
        "LLM_with_pdf_context_comparison(\"Whirlpool\", \"Electrolux\", context_whirlpool, context_electrolux, chat_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End of the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: After completed course we can flush and unmount drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First flush and unmount drive after we are done,\n",
        "# but to re-mount with new login we may need to remove dir manually first\n",
        "if use_drive:\n",
        "  drive.flush_and_unmount()\n",
        "  !rm -rf /content/drive\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FkZyvXSmxl3S",
        "uczbePAtomja",
        "S0W6WCuAdIJm",
        "Tad6kXJ_xtj6",
        "BUoFq1J5nTOE",
        "q97YM4YIncHT",
        "z9ins43Unf0Y"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "cb01f1dbab416340528beb9003d373e701839649ec4e19c51caaaa46d79e9db4"
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 ('pe_short_course')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
