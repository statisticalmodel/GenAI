{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZyvXSmxl3S"
      },
      "source": [
        "# Generative AI - Combient Mix AB / Silo AI for Patricia AB 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sF2Up6-RmhA"
      },
      "source": [
        "Prompt Engineering (PE) is the primary vehicle for guiding generative AI models towards stable applications. It is particularly applicable to Large Language Models (LLMs) and involves formulating prompts and prompting schemas in order to retrieve appropriate responses to queries. It is essential for meaningful interactions with LLMs.\n",
        "\n",
        "This notebook provides a hands-on introduction to PE for end users with a basic understanding of the Python programming language. No advanced coding and no technical background knowledge of generative AI or LLM is required. The notebook is divided into three sections, each corresponding to material covered in the first of two live seminars. There will be ~40 minutes available in order to go through and complete each section.\n",
        "<br />\n",
        "<br />\n",
        "\n",
        "***\n",
        "\n",
        "**Structure**\n",
        "\n",
        "Exercise **1 & 2 during the first session** (~ 40 minutes per exercise) and exercise **3 during the second session** (~ 60 minutes)\n",
        "\n",
        "\n",
        "1.) First, we'll explore the basics of prompting LLMs and the importance of PE. We will see examples of some important and cutting edge techniques for prompting efficiently and with intent\n",
        "* 0 / 1 / few-shot prompting,\n",
        "* Role-Task-Format (RTF),\n",
        "* In-Context-Learning (ICL),\n",
        "* Chain-of-Thought (CoT) and\n",
        "* Tree-of-Thought (ToT)\n",
        "\n",
        "2.) We'll learn how to integrate and utilize existing tools for specific tasks. We focus on some of the tools available via enabling plugins for GPT-4 at [OpenAI chat interface](https://chat.openai.com/). Example tasks include \n",
        "  * **Searching the internet for recent information** LLMs are generally pre-trained and thus have a knowledge cutoff at the time when training stopped. The latest GPT-4 model of OpenAI for example has a knowledge cutoff at January 2022. This means that it was not trained on any data produced after that time, so if we want to include updated information this must be done by fetching the information and providing it as context together with our query.\n",
        "  * **Using your own PDF documents as a knowledge base** (plugin: AskYourPDF). There are several plugins available for GPT-4 which provide the capability of uploading a PDF document and use it as a reference for interacting with the LLM.\n",
        "\n",
        "3.) The third exercise is more programming heavy and involves working with API calls. We'll guide you through it! This allows us to perform more complex tasks and write our own custom functions utilizing the power of the LLM. We introduce the Python library [Langchain](https://python.langchain.com/docs/get_started/introduction.html) and some of its tools for manipulating prompts. This allows for more intricate problem-solving as well as gaining control over the reliability of the output. We will see how to use this in order to accomplish Retrieval Augmented Generation (RAG) from our own knowledge base (the PDF document). This exercise demonstrates how the plugins used in exercise 2 work under the hood. Examples of what we can achieve include\n",
        "\n",
        "* constraining the system behaviour, for example to mitigate hallucinations from the LLM\n",
        "* Retrieval Augmented Generation from an external knowledge base such as the internet or a collection of documents\n",
        "\n",
        "This provides a deeper understanding of the fundamental components essential for constructing advanced generative AI systems.\n",
        "\n",
        "***\n",
        "\n",
        "**Note:**\n",
        "\n",
        "In order to run this notebook properly you will need\n",
        "\n",
        "* **Gmail account** - Follow the provided instructions to download the notebook to your GDrive, so that you can edit and save it freely.\n",
        "* **Google GDrive access** - enable the app `Google Colaboratory`. The process is highlighted in the accompanyiing reading material, but you can also ask ChatGPT a question like this by posing the following query <span style=\"color:blue;\">how do I enable google colab for the first time?</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FflnrWlYmoZ"
      },
      "source": [
        "> Run the below code blocks to install necessary packages.\n",
        ">\n",
        "> Notebook code blocks can be executed via either: \n",
        "> * **shift + enter**: executes current code block and moves to the cell below\n",
        "> * **control + enter**: executes current code block"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uczbePAtomja"
      },
      "source": [
        "## Environment setup\n",
        "\n",
        "Here we set up the environment and make sure we can access data via Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the code below to load the GenAI map we will be using during this course. The code is fetching a map called GenAI from our Github repository. Click on plus next to the cell to run the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "use_drive = False\n",
        "\n",
        "if use_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  %cd /content/drive/MyDrive\n",
        "\n",
        "  !git clone https://github.com/statisticalmodel/GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now view the files by clicking on the map and follow the path down in the GenAI short course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Image of GitClone](https://github.com/statisticalmodel/GenAI/blob/main/GenAI%20short%20course/GitClone.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Packages & Imports\n",
        "\n",
        "Installing and importing necessary packages/modules. Note that these should be installed only in a virtual environment when using the Colab Notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the below code block to install some of the Python libraries which are required for running the Notebook.\n",
        "\n",
        "Note that this may take up to ~15 seconds to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: All packages below are not necessary and should be cleaned after completion of exercises**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -qU transformers \\\n",
        "    --upgrade huggingface_hub \\\n",
        "    -q sentencepiece \\\n",
        "    -q accelerate \\\n",
        "    -q tiktoken \\\n",
        "    -q openai \\\n",
        "    -q langchain \\\n",
        "    -q sentence-transformers \\\n",
        "    -q jq \\\n",
        "    -q faiss-cpu \\\n",
        "    -q pypdf \\\n",
        "    -q wikipedia \\\n",
        "    -q duckduckgo-search \\\n",
        "    -q colorama \\\n",
        "    -q PyMuPDF "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the below code block to import the necessary library modules used in the notebook.\n",
        "\n",
        "Note that this may take up to ~20 seconds to complete."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: All packages below are not necessaty and should be cleaned after completion of exercises**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
            "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Some system and base modules\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from timeit import default_timer as timer\n",
        "from typing import Any, List, Dict, Optional, Type\n",
        "import getpass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import requests\n",
        "from PIL import Image\n",
        "import io\n",
        "import re\n",
        "from time import time\n",
        "from termcolor import colored\n",
        "\n",
        "# NLP modules\n",
        "import torch\n",
        "import openai\n",
        "from openai.embeddings_utils import cosine_similarity\n",
        "from huggingface_hub import login\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModel\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader,JSONLoader, UnstructuredMarkdownLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.schema.document import Document\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import fitz\n",
        "\n",
        "# Other modules\n",
        "from colorama import Fore, Back, Style\n",
        "import wikipedia\n",
        "\n",
        "# Modules for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import helper functions used in the Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the code block below to import customized helper functions used in the notebook. These functions have been written to, for example, process PDF files and reduce clutter in the notebook by using explicit code. All of the imported functions are located in the file `helper_functions.py` where you may explore their definitions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/GenAI/GenAI short course/')\n",
        "\n",
        "from helper_functions import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setting the access key for OpenaAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**The OpenAI API key can be set manually in the notebook by runnning the code cell block below.**\n",
        "\n",
        "You can optionally set it as an environment variable; by typing the following in your Mac terminal\n",
        "```\n",
        "export OPENAI_API_KEY=sk-...\n",
        "```\n",
        "or if you are using windows 10 you can type the following in a Command window\n",
        "```\n",
        "set OPENAI_API_KEY=sk-...\n",
        "```\n",
        "Observe the lack of space in the value designations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: Delete the API access key below before pushing to repo etc.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sk-...\n",
        "\n",
        "# Here we can set the OpenAI api access key manually in case it fails to load from the environment.\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    api_key = getpass.getpass(\"Enter OpenAI API Key here\")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "else:\n",
        "  print(f\"OPENAI_API_KEY fetched from environment!\")\n",
        "\n",
        "\n",
        "#print(f\"The Open AI access key is given by: \\n\\n {os.environ['OPENAI_API_KEY']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q97YM4YIncHT"
      },
      "source": [
        "# Hands-On 3: Customized Tools for Extended LLM Functionality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction to Interacting with ChatGPT via the OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiaTQss8n14Q"
      },
      "source": [
        "Using LangChain agents for tasks. Example themes for exercises:\n",
        "\n",
        "* Search the internet, e.g., Wikipedia, DuckDuckGo or Google Custom Search APIs.\n",
        "  - Useful and easy to implement and understand at a basic level.\n",
        "  - Allows to go beyond the Wiki app and have more control.\n",
        "* Search a knowledge base, e.g., PDF, Excel or plain text files.\n",
        "  - Useful for many purposes. Builds on previous exercises and showcase the behind the scenes work involved in the plugins.\n",
        "  - Requires that we pre-vet or supply the material since there may be format issues which can't be dealt with in a timely fashion otherwise.\n",
        "* Generate and understand code - maybe some simple examples, but only if there is time and we can think of some relatively simple and instructive examples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Why should you use Python and API?**\n",
        "\n",
        "Using the OpenAI API through Python, instead of their online chat interface, gives us several advantages, especially when we want to build stronger and custom made solutions. One of the big perks of using Python is automation. Python is great at taking care of repeating or tricky tasks for us, which is helpful when we have a lot of such tasks to handle.\n",
        "\n",
        "With the help of python we can also get the OpenAI models to work well with other tools and software we have. Plus, it lets us adjust things to work the way we want, making our interaction with OpenAI more custome made.\n",
        "\n",
        "Python has a lot of advanced features which help us use the OpenAI service to its full potential. This means we can do a wider variety of tasks. In this part of the course, we will look into how a model can get access to more reasent data trough access to internet and how you can extract knowledge from pdfs.\n",
        "\n",
        "Additionally, using python helps us keep track of any changes made along the way, which is great for teamwork. Python also ensures that every time we run a task, it gives us consistent results, making it easier to check if something goes wrong and fix it. This reliability helps make our solutions stronger and more dependable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will start by using the library called `langchain` to call on the OpenAI model. It is a simpler and more user-friendly way to interact with the model, making it a great choice for beginners or for those looking to get things done quickly. This method provides us with a `temperature` parameter that determines the randomness or stochastic nature of the output the model will give us. A temperature of 1 makes the model more random, while a temperature close to 0 makes the model more deterministic with the same output for repeated queries. Most people prefer the output to be somewhat creative in nature with a temperature value of around 0.7, which is the default value set for the OpenAI models. You can try changing this value and experience how the output changes for the same query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the code snipped below we call on the API. Model is specified via `model` on the second row. A complete list of available models to choose from can be found [here](https://platform.openai.com/docs/models/overview). We will start by using one of the GPT-3.5 turbo models. \n",
        "\n",
        "* **gpt-3.5-turbo** uses the latest available model version of GPT-3.5. Allows for up to **4,097 tokens** as input and output\n",
        "    * **gpt-3.5-turbo-0613** is the latest version of GPT-3.5, currently the same as above\n",
        "    * **gpt-3.5-turbo-16k** is a version of GPT-3.5 with longer context window, up to **16,385 tokens**\n",
        "\n",
        "</b>\n",
        "\n",
        "\n",
        "* **gpt-4** uses the latest available model version of GPT-4. Allows using **8,192 tokens** as input and output\n",
        "    * **gpt-4-0613** is the latest version of GPT-4, same as above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "# This createss an instance of the model interface which we can subsequently call on\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model=\"gpt-3.5-turbo-0613\",\n",
        "    temperature=0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB 1: The GPT-3.5 model usually takes between 15 to 45 seconds to respond. If it takes more than a minute, stop the execution and rerun it.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB 2: This is one option for implementing the LLM call, which is nice due to its simple and intuitive format. At the bottom of this section we explore using the OpenAI API directly as an alternative option, which provides even greater control of the input/output.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also specify messages according to a schema used by various functions in the `langchain` library. This will facilitate some of the more advanced functionalities we will make use of later. Messages in a chat conversation are generally provided as a `list` (inside square brackets) of conversation turns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.schema import (\n",
        "    SystemMessage,\n",
        "    HumanMessage,\n",
        "    AIMessage\n",
        ")\n",
        "\n",
        "# The system prompt will be placed at the top of every message and should set overall system behaviour\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "\n",
        "# The user prompt is what would be written in the chat interface, your query\n",
        "user_prompt = \"What do you know about Patricia Industries from Sweden?\"\n",
        "\n",
        "# These are collected into a message list of\n",
        "#\n",
        "# *  SystemMessage - the system prompt\n",
        "# *  HumanMessage - the user query\n",
        "# *  AIMessage - the bot response, in case you wish to continue on a conversation\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_prompt),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant.'),\n",
              " HumanMessage(content='What do you know about Patricia Industries from Sweden?')]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We can check what the messages look like by printing it out\n",
        "messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once we have a list of messages in the above format it is easy to call the OpenAI model and get a response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patricia Industries is an investment company based in Sweden. It is a part of the Swedish industrial group, Investor AB. \n",
            "\n",
            "Patricia Industries focuses on long-term investments in companies with strong market positions and growth potential. Its investment strategy is centered around three main areas: Nordics, DACH (Germany, Austria, and Switzerland), and North America. \n",
            "\n",
            "The company seeks to actively develop and support its portfolio companies by providing strategic guidance and expertise. It aims to create value through operational improvements, strategic acquisitions, and investments in innovation and sustainability.\n",
            "\n",
            "Patricia Industries has a diverse portfolio of companies across various industries, including healthcare, industrials, consumer goods, and technology. Some notable investments include Mölnlycke Health Care, Aleris, Permobil, BraunAbility, and Vectura Group.\n",
            "\n",
            "Overall, Patricia Industries is known for its long-term approach to investing, commitment to sustainable growth, and focus on building strong partnerships with its portfolio companies.\n"
          ]
        }
      ],
      "source": [
        "# Here we collect the ouput from the chat model in a variable response\n",
        "response = chat_model(messages)\n",
        "\n",
        "# We can print out the response by calling on its content using a .content\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The full response is actually formatted as a `AIMessage` as we can see by printing out the full response without using the print function. Observe that if don not use the formatting of the `print` fucntion you will see linebreak characters such as `\\n` appearing in the message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Patricia Industries is an investment company based in Sweden. It is a part of the Swedish industrial group, Investor AB. \\n\\nPatricia Industries focuses on long-term investments in companies with strong market positions and growth potential. Its investment strategy is centered around three main areas: Nordics, DACH (Germany, Austria, and Switzerland), and North America. \\n\\nThe company seeks to actively develop and support its portfolio companies by providing strategic guidance and expertise. It aims to create value through operational improvements, strategic acquisitions, and investments in innovation and sustainability.\\n\\nPatricia Industries has a diverse portfolio of companies across various industries, including healthcare, industrials, consumer goods, and technology. Some notable investments include Mölnlycke Health Care, Aleris, Permobil, BraunAbility, and Vectura Group.\\n\\nOverall, Patricia Industries is known for its long-term approach to investing, commitment to sustainable growth, and focus on building strong partnerships with its portfolio companies.')"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Giving ChatGPT a memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can incorporate the `AIMessage` response in a new series of messages and also ask a follow up question if you wish. This provides the illusion of the chatmodel having a memory and being able to continue a conversation for a few turns, as you have seen while working in the browser environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant.'),\n",
              " HumanMessage(content='What do you know about Patricia Industries from Sweden?'),\n",
              " AIMessage(content='The strategies for adjusting the Front Seat Height on Slipstream Single-Sided Forks are similar in both sets of instructions. Both sets mention the use of a 5/8\" Open End Wrench and a screwdriver as the required tools for the adjustment. \\n\\nAdditionally, both sets of instructions mention the steps of removing the caster, removing the E-Ring, removing the axle, placing the axle in the alternate hole, replacing the E-Ring, and finally replacing the caster. \\n\\nHowever, there are some differences in the instructions. The first set of instructions mentions the need for protective eyewear when removing the E-Ring, while the second set does not. The second set of instructions also includes additional tools such as a Wood Block, Drafting Triangle, Rubber Mallet, and Ruler or Measuring Tape, which are not mentioned in the first set of instructions. \\n\\nFurthermore, the second set of instructions includes warnings about using identical axle holes on both sides of the chair and the need to reapply a locking and sealing coating after every fourth adjustment, which are not mentioned in the first set of instructions.'),\n",
              " HumanMessage(content='Can you say something more about the investment approach?')]"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's ask a follow-up question which refers to the previous conversation without explicitly mentioning e.g., Patricia Industries\n",
        "follow_up_question = \"Can you say something more about the investment approach?\"\n",
        "\n",
        "# We can now add the respinse we got from the first query together with our follow-up question\n",
        "messages = messages + [response, HumanMessage(content=follow_up_question)]\n",
        "messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patricia Industries is an investment firm based in Sweden that follows a long-term and active ownership approach. They focus on investing in companies with strong growth potential and a sustainable business model. \n",
            "\n",
            "Their investment approach involves partnering with companies and providing them with the necessary resources, expertise, and support to drive growth and value creation. They aim to be a strategic and value-adding owner, working closely with management teams to develop and execute on growth strategies.\n",
            "\n",
            "Patricia Industries has a sector-agnostic approach, meaning they invest in a wide range of industries including healthcare, technology, industrials, and consumer goods. They seek to identify companies that have a competitive advantage, innovative products or services, and a strong market position.\n",
            "\n",
            "In addition to financial capital, Patricia Industries also brings operational expertise and a network of industry contacts to help their portfolio companies succeed. They have a long-term investment horizon and are committed to building sustainable businesses for the future.\n",
            "\n",
            "Overall, Patricia Industries' investment approach is characterized by active ownership, strategic partnerships, and a focus on long-term value creation.\n"
          ]
        }
      ],
      "source": [
        "# Here we collect the ouput from the chat model in a variable response\n",
        "response = chat_model(messages)\n",
        "\n",
        "# We can print out the response by calling on its content using a .content\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By continuing in this fashion we can record a conversation history which we send to the chat model as long as the total text content does not exceed the models content limit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### OPTIONAL: Using the OpenAI API directly instead of the `langchain` wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below code snippet calls the OpenAI API directly, which allow to access **all** of the available input and output options. While this approach offers a higher degree of control, it may not be as easy to use or straightforward as the functions from LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Patricia Industries is a Swedish investment company that operates as a part of Investor AB, one of Sweden's largest industrial holding companies. Patricia Industries focuses on long-term investments in companies across various sectors, including healthcare, technology, and consumer goods. The company aims to support and develop its portfolio companies by providing strategic guidance and financial resources. Patricia Industries has a strong commitment to sustainability and responsible business practices.\n"
          ]
        }
      ],
      "source": [
        "# Needs to be set for the OpenAI API to be callable\n",
        "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "\n",
        "# Model and parameters (almost all that are available, logit_bias not included). Parameters model and messages are required\n",
        "model = \"gpt-3.5-turbo-0613\"\n",
        "max_tokens = 1024         # Max nr of tokens to generate in the chat completion, limited by model choice\n",
        "temperature = 0.7         # Value in (0, 2). Sampling temperature for stochastic nature in response\n",
        "top_p = 1                 # Vale in (0, 1). Nucleus sampling, optional to temperature, considers tokens comprising top_p probability mass\n",
        "frequency_penalty = 0     # Value in (-2, 2). Positive value penalizes new tokens based on frequency in text so far, to decrease likelihood of repeating sentences\n",
        "presence_penalty = 0      # Value in (-2, 2). Positive value penalizes new tokens if appeared in text so far, to increase likelihood of switching to new topics\n",
        "n=1                       # How many completion options to generate for each message\n",
        "stream = False            # Boolean value which can allow for streaming response\n",
        "\n",
        "\n",
        "\n",
        "function_call = \"none\"      # 'auto' if call function or generate message, supply {\"name\": my_func}\n",
        "functions = [{\"name\": \"Name\", \"description\": \"semantic description of what functions do\", \"parameters\": {\"type\": \"object\", \"properties\": {}}}]\n",
        "\n",
        "# Messages are similar to the langchain format but different\n",
        "# Other values for the key 'role' are 'assistant' and 'function'\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "user_prompt = \"What do you know about Patricia Industries from Sweden?\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_prompt},\n",
        "    {\"role\": \"user\", \"content\": user_prompt},\n",
        "    ]\n",
        "\n",
        "\n",
        "# API call\n",
        "response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            temperature=temperature,\n",
        "            max_tokens=max_tokens,\n",
        "            top_p=top_p,\n",
        "            frequency_penalty=frequency_penalty,\n",
        "            presence_penalty=presence_penalty,\n",
        "            n=n,\n",
        "            stream=stream,\n",
        "            function_call=function_call,\n",
        "            functions=functions\n",
        "        )\n",
        "\n",
        "\n",
        "# Printing the response requires a slighty less intuitive format\n",
        "print(response.choices[0].message[\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can take a look at the complete response without simplifying or changing the output in any way. When you print the information from the code below, you will see that the response includes details about token usage and other aspects. These details are important as they help you estimate the cost of running queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<OpenAIObject chat.completion id=chatcmpl-8FgDJVOpRPp7WbmW3jpYM3JS8TfHx at 0x2b186c810> JSON: {\n",
              "  \"id\": \"chatcmpl-8FgDJVOpRPp7WbmW3jpYM3JS8TfHx\",\n",
              "  \"object\": \"chat.completion\",\n",
              "  \"created\": 1698747881,\n",
              "  \"model\": \"gpt-3.5-turbo-0613\",\n",
              "  \"choices\": [\n",
              "    {\n",
              "      \"index\": 0,\n",
              "      \"message\": {\n",
              "        \"role\": \"assistant\",\n",
              "        \"content\": \"Patricia Industries is a Swedish investment company that operates as a part of Investor AB, one of Sweden's largest industrial holding companies. Patricia Industries focuses on long-term investments in companies across various sectors, including healthcare, technology, and consumer goods. The company aims to support and develop its portfolio companies by providing strategic guidance and financial resources. Patricia Industries has a strong commitment to sustainability and responsible business practices.\"\n",
              "      },\n",
              "      \"finish_reason\": \"stop\"\n",
              "    }\n",
              "  ],\n",
              "  \"usage\": {\n",
              "    \"prompt_tokens\": 55,\n",
              "    \"completion_tokens\": 79,\n",
              "    \"total_tokens\": 134\n",
              "  }\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Connecting Language Models to the Internet: A way to Expanded Knowledge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will explore two simple open source options for adding internet content as context for our query to the LLM:\n",
        "\n",
        "* The Wikipedia API which allow us to read in a relevant Wikipedia information to get factual information on a specific topic\n",
        "* The DuckDuckGo API which allows us to search updated information on a topic, to use as context for our query"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wikipedia API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If we wish to look up or summarize information from Wikipedia and we know the title of the page we're interested in, we can easily use the Wikipedia API to grab the content from that page. However, it's important to know that the Wikipedia API isn't designed for broad searches like a regular search engine. It works best when the search term you enter matches a Wikipedia page title quite closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's try by asking a question\n",
        "query = \"What is Patricia Industries\"\n",
        "\n",
        "# Try searching with a general question. The how_many argument specifies how many letters should be printed from the response\n",
        "# Here, we are calling a function named wiki_search_api with two arguments; query and how_many\n",
        "result_wiki = wiki_search_api(query, how_many=1000)\n",
        "\n",
        "# We print the result to the console. This will display the first 1000 characters of the Wikipedia page or information that relates to \"Patricia Industries\" \n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we see the answer is not related to Patricia Industries at all. By searching instead for a keyword we can increase the chances of obtaining a more relevant answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Investor_AB\n",
            "Investor AB is a Swedish investment and holding company, often considered a de facto conglomerate. One of Sweden's largest companies, Investor AB serves as the investment arm of the prominent Swedish Wallenberg family; the family's companies are involved in a variety of industries, of which the primary industries are pharmaceuticals, telecommunications and industry.Investor AB is Sweden's most valuable publicly traded company; it has major or controlling holdings in several of Sweden's other largest companies. It has major investments worldwide through Patricia Industries and EQT Partners.\n",
            "\n",
            "\n",
            "== History ==\n",
            "Investor AB was established in Stockholm when new Swedish legislation made it more difficult for banks to own stocks in industrial companies on a long-term basis. The shareholdings of the Wallenberg family bank, Stockholms Enskilda Bank, were transferred to Investor AB, a newly formed industrial holding company spun off from the bank, which would serve as the family's investment arm h\n"
          ]
        }
      ],
      "source": [
        "# This is the keyword you want to search for on Wikipedia.\n",
        "query_keyword = \"Patricia Industries\"\n",
        "\n",
        "# By searching on the keyword we obtain a better match. \n",
        "result_wiki = wiki_search_api(query_keyword, how_many=1000)\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can customize this somewhat by adding a function which extracts the keyword from a sentence by looking for the phrase `keyword: <KEYWORD>`. This has some limitiations though since it requires us to include this kind of formatted phrase in our query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Investor_AB\n",
            "Investor AB is a Swedish investment and holding company, often considered a de facto conglomerate. One of Sweden's largest companies, Investor AB serves as the investment arm of the prominent Swedish Wallenberg family; the family's companies are involved in a variety of industries, of which the primary industries are pharmaceuticals, telecommunications and industry.Investor AB is Sweden's most valuable publicly traded company; it has major or controlling holdings in several of Sweden's other largest companies. It has major investments worldwide through Patricia Industries and EQT Partners.\n",
            "\n",
            "\n",
            "== History ==\n",
            "Investor AB was established in Stockholm when new Swedish legislation made it more difficult for banks to own stocks in industrial companies on a long-term basis. The shareholdings of the Wallenberg family bank, Stockholms Enskilda Bank, were transferred to Investor AB, a newly formed industrial holding company spun off from the bank, which would serve as the family's investment arm h\n"
          ]
        }
      ],
      "source": [
        "query_keyword = \"Patricia Industries\"\n",
        "query_with_keyword = f\"What is keyword: {query_keyword}\"\n",
        "\n",
        "# By searching on the keyword we obtain a better match. \n",
        "result_wiki = wiki_search_api(find_keyword(query_with_keyword), how_many=1000)\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own keyword. The more closely this matches an existing Wikipedia title page, the more likely you are of finding what you look for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Ellipsis\n",
            "The ellipsis ... (; also known informally as dot dot dot) is a series of dots that indicates an intentional omission of a word, sentence, or whole section from a text without altering its original meaning. The plural is ellipses. The term originates from the Ancient Greek: ἔλλειψις, élleipsis meaning 'leave out'.Opinions differ as to how to render ellipses in printed material. According to The Chicago Manual of Style, it should consist of three periods, each separated from its neighbor by a non-breaking space: . . .. According to the AP Stylebook, the periods should be rendered with no space between them: .... A third option is to use the Unicode character U+2026 … HORIZONTAL ELLIPSIS.\n",
            "\n",
            "\n",
            "== Background ==\n",
            "The ellipsis is also called a suspension point, points of ellipsis, periods of ellipsis, or (colloquially) \"dot-dot-dot\". Depending on their context and placement in a sentence, ellipses can indicate an unfinished thought, a leading statement, a slight pause, an echoing voice, or a ner\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Choose a keyword for your search. It's better to pick a keyword that closely matches a Wikipedia page title.\n",
        "# This increases your chances of finding the information you're looking for. If you can't come up with something try query_keyword = \"rubberducking\"\n",
        "query_keyword = \"...\"\n",
        "\n",
        "# Step 2: Now, let's search Wikipedia using the keyword you chose. \n",
        "# The 'how_many' part tells the program how many characters of the result you want to see.\n",
        "# In this case, we've set it to 1000, so you'll see the first 1000 characters of the Wikipedia page.\n",
        "result_wiki = wiki_search_api(query_keyword, how_many=1000)\n",
        "\n",
        "# Step 3: Print the result to the screen so you can read it.\n",
        "print(result_wiki)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicitly using Wikipedia facts for the purpose of answering a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Setting up the Model\n",
        "# We're preparing to use a specific AI model by setting some initial parameters.\n",
        "# One of these parameters is 'temperature', which affects how creative or strict the model's responses will be.\n",
        "# A lower temperature like 0.0 makes the model more focused and less random in its responses.\n",
        "temperature = 0.0\n",
        "\n",
        "# Now we create an instance of the ChatOpenAI model, which we'll use to generate responses.\n",
        "# We're specifying which model to use ('gpt-3.5-turbo') and providing our OpenAI API key to authorize access.\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=temperature\n",
        ")\n",
        "\n",
        "\n",
        "# Step 2: Defining a Function to Use the Model\n",
        "# We're creating a function to make it easier to use the model with different questions and keywords.\n",
        "# This function will search Wikipedia for context, then ask the model to answer a query based on that context.\n",
        "def LLM_with_wiki_prompt(query: str, query_keyword: str, how_many: int=2000, temperature: float=0.0):\n",
        "\n",
        "    # Retrieve serch results based the query\n",
        "    # This line searches Wikipedia using the keyword provided, and gets up to 2000 characters of text as context.\n",
        "    context = wiki_search_api(query_keyword, how_many=how_many)\n",
        "\n",
        "    # System prompt\n",
        "    # We're setting up a prompt to instruct the model on how to approach answering the query.\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful investment management expert.\n",
        "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
        "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
        "    Then proceed to answer in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt inputs the query and telling the model to consider the Wikipedia context we found.\n",
        "    user_prompt = f\"\"\"\n",
        "    Consider carefully the following wiki facts as context.\n",
        "    context: {context}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following query and summarize your response.\n",
        "    query: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context,\n",
        "    then return those bullets verbatim and do not try to rephrase them.\n",
        "    \"\"\"\n",
        "\n",
        "    # We're arranging the system and user prompts into a list of messages to send to the model.\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "\n",
        "\n",
        "    # We ask the model to generate a response based on the messages, and print that response.\n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Investor_AB\n",
            "- Investor AB has major investments worldwide through Patricia Industries.\n",
            "- Patricia Industries is a subsidiary or division of Investor AB.\n",
            "- Patricia Industries is the investment arm of Investor AB for its global investments.\n",
            "- Patricia Industries is involved in various industries, including pharmaceuticals, telecommunications, and industry, just like Investor AB.\n",
            "- Patricia Industries is responsible for managing and overseeing Investor AB's major investments outside of Sweden.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is the relationship between Patricia Industries and Investor AB?\"\n",
        "query_keyword = \"Patricia Industries\"\n",
        "\n",
        "LLM_with_wiki_prompt(query=query, query_keyword=query_keyword, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query and relevant keyword. The more closely the keyword matches an existing Wikipedia title page, the more likely it is that the LLM will be able to answer your query based on Wikipedia facts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://en.wikipedia.org/wiki/Ellipsis\n",
            "The ellipsis, also known as dot dot dot, is a series of dots that indicates an intentional omission of a word, sentence, or whole section from a text without altering its original meaning. It can be rendered as three periods with a non-breaking space between them ( . . . ) according to The Chicago Manual of Style, or with no space between them (....) according to the AP Stylebook. Another option is to use the Unicode character U+2026 (… HORIZONTAL ELLIPSIS).\n",
            "\n",
            "Ellipses can indicate an unfinished thought, a leading statement, a slight pause, an echoing voice, or a nervous or awkward silence. They can be used to trail off into silence or suggest melancholy or longing when placed at the end of a sentence.\n",
            "\n",
            "The most common forms of an ellipsis are a row of three periods or the horizontal ellipsis glyph. Style guides have their own rules for the use of ellipses, such as using spaces around the periods or putting them together with a space before and after the group.\n",
            "\n",
            "Whether an ellipsis at the end of a sentence needs a fourth dot to finish the sentence is a matter of debate. Some style guides, like Chicago and APA, advise using a fourth dot, while others do not.\n"
          ]
        }
      ],
      "source": [
        "# Assign the query string, which is the question you want to answer.\n",
        "query = \"...\"\n",
        "\n",
        "# Assign the query \"keyword\" string, which is the keyword you want to use to fetch relevant context from Wikipedia.\n",
        "query_keyword = \"...\"\n",
        "\n",
        "# Now, you are calling the function LLM_with_wiki_prompt which was defined earlier.\n",
        "# You are passing the query, query_keyword, and temperature (defined earlier) as arguments to the function.\n",
        "# This function is expected to search Wikipedia using the query_keyword, \n",
        "# then use the model to answer the query based on the Wikipedia information,\n",
        "# and finally the function prints out the model's response.\n",
        "LLM_with_wiki_prompt(query=query, query_keyword=query_keyword, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DuckDuckGo search API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The [DuckDuckGo browser](https://duckduckgo.com/) provides an open source API which can be used to browse and search the internet. This returns a search response to a query and can be useful for incorporating a small amount of updated information as context to your LLM queries. More advanced search functionalities are possible but quickly become more technically involved for the user, so for the purpose of this short course we restrict to the basic search functionality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Investor AB (publ) is a venture capital firm specializing in mature, middle market, buyouts and growth capital investments. It is operating through four business areas including core, private equity, operating, and financial investments. 920 followers $18.97 0.00 ( 0.00%) 3:06 PM 10/06/23 Pink Limited Info | $USD | Delayed Summary Ratings Financials Earnings Dividends Valuation Growth Profitability Momentum Peers Options Charting... Investor AB is a Sweden-based industrial holding company. Its operations are divided into three business segments: Listed Core Investments, EQT and Patricia Industries. The Listed Core Investments segment consists of listed holdings, which embrace ABB, AstraZeneca, Atlas Copco, Electrolux, Ericsson, Husqvarna, Nasdaq, Saab, SEB, Sobi and Wartsila. Conclusion: Summary: I'm long Investor AB because of these reasons: 100 years of successful investing. Investor AB has outperformed the Swedish index. The Wallenberg family controls the company and has proven themselves good capital allocators. The Wallenbergs and management think like owners. gorodenkoff Investor AB ( OTCPK:IVSXF) is trading at close to book value, while the best period to buy its shares is when it's trading at a discount. Company Overview Investor AB is one of...\n"
          ]
        }
      ],
      "source": [
        "query = \"What's the latest business news about Investor AB?\"\n",
        "\n",
        "# Note that the retrieved responses are not necessarily the same every time we run a search.\n",
        "response = get_search_results_ddg(query, max_results=50)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that you receive a collection of relevant snippets to your query and not an actual answer. To collect the retrieved information into an answer we can use the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Explicitly using the query for the purpose of answering a question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "query = \"What's the latest business news about Investor AB and its board?\"\n",
        "\n",
        "# Let's prepare to use the response as context for querying the LLM\n",
        "context_ddg = get_search_results_ddg(query, max_results=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Investor AB INVEb.ST LSEG Official Data Partner Latest Trade trading lower199.56SEK Change -0.42 % Change -0.21%Negative As of Oct 23, 2023. STOCKHOLM--Investor AB Chief Executive Johan Forssell will step down from his position in conjunction with the annual general meeting on May 7, the company said Friday. Through a consultancy agreement, Forssell will transition to a new role that will see him assigned to Investor as an industrial advisor with a focus on industrial companies ... Investor AB : News, information and stories for Investor AB | Nasdaq Stockholm: INVE B | Nasdaq Stockholm ... Business Leaders. Sectors. All our articles. Most Read News. ... CEO Johan Forssell will leave Investor in May 2024 in a new role with focus on Oct. 20. AQ Johan Forssell to Leave from Investor AB as President and Director, Effective 7 ... 920 followers $18.97 0.00 ( 0.00%) 3:06 PM 10/06/23 Pink Limited Info | $USD | Delayed Summary Ratings Financials Earnings Dividends Valuation Growth Profitability Momentum Peers Options Charting... Latest news about Investor AB (publ) Investor AB(OM:INVE A) dropped from S&P Global BMI Index ... Investor AB is a Sweden-based industrial holding company. Its operations are divided into three business segments: Listed Core Investments, EQT and Patricia Industries. The Listed Core Investments segment consists of listed holdings, which embrace ...\n"
          ]
        }
      ],
      "source": [
        "print(context_ddg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we are ready to implement the internet search functionality together with the LLM for answering our queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This createss an instance of the model interface which we can subsequently call on.\n",
        "# Here, you can experiment with the temperature parameter which affects the randomness of the model's output.\n",
        "temperature = 0.0\n",
        "\n",
        "# Initializing the chat_model object with the specified parameters.\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'], \n",
        "    model=\"gpt-3.5-turbo\", # Specifying the model version to be used.\n",
        "    temperature=temperature # Setting the temperature parameter for the model.\n",
        ")\n",
        "\n",
        "#  Defining a function to interact with the LLM using the same prompt template but varying query and context.\n",
        "def LLM_with_search_prompt(query: str, max_results: int=10, temperature: float=0.0):\n",
        "\n",
        "   # Retrieving search results based on the query provided, limited to max_results.\n",
        "    context = get_search_results_ddg(query, max_results=max_results)\n",
        "\n",
        "    # Defining the System Prompt to instruct the model.\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful investment management expert.\n",
        "    You are logical, methodical and always find the best and most relevant answer to a query.\n",
        "    Break down the problem, objects, numbers and logic before starting to answer the query.\n",
        "    Then proceed to answer in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # Constructing the user prompt with the retrieved context and specified query.\n",
        "    user_prompt = f\"\"\"\n",
        "    One of our portfolio companies have handed us these news reports as context.\n",
        "    context: {context}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following query and summarize your response.\n",
        "    query: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context,\n",
        "    then return those bullets verbatim and do not try to rephrase them.\n",
        "    \"\"\"\n",
        "\n",
        "    # Creating a list of message objects for interaction with the model.\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "\n",
        "    # Calling on the chat_model with the constructed messages and specified temperature, then printing the model's response.\n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the function below will print the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Investor AB CEO Johan Forssell will step down from his position in conjunction with the annual general meeting on May 7.\n",
            "- Johan Forssell will leave Investor AB in May 2024 in a new role with a focus on October 20.\n",
            "- Investor AB is a venture capital firm specializing in mature, middle market, buyouts, and growth capital investments.\n",
            "- Investor AB operates through four business areas including core, private equity, operating, and financial investments.\n"
          ]
        }
      ],
      "source": [
        "LLM_with_search_prompt(query=query, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query. The context will be automatically included via the prompt function defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the given context, it seems that the query is missing. There is no specific question mentioned in the context. Therefore, I cannot provide a response or summarize the answer.\n"
          ]
        }
      ],
      "source": [
        "# Write your own query and retrieve search results from that as context for the LLM\n",
        "query = \"...\"\n",
        "\n",
        "LLM_with_search_prompt(query=query, temperature=temperature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF as knowledge base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We start by providing the location and names of the PDF files which we will be exploring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "N8E7XBwp_rho"
      },
      "outputs": [],
      "source": [
        "# Absolute or relative path to folder where the PDF files are located\n",
        "DATAPATH = \"/content/drive/MyDrive/GenAI/GenAI short course/\"\n",
        "\n",
        "# Name of the PDF files (without the trailing .pdf file endings)\n",
        "Patricia_permobil_manual = \"Patricia_permobil_manual\"\n",
        "caterpillar_10k = \"Caterpillar-10k\"\n",
        "whirlpool_10k = \"whirlpool-10k\"\n",
        "electrolux_ann_rep = \"Electrolux-annual-report\"\n",
        "\n",
        "# We collect all the PDF file names in a list\n",
        "pdf_files = [\n",
        "    Patricia_permobil_manual,\n",
        "    caterpillar_10k,\n",
        "    whirlpool_10k,\n",
        "    electrolux_ann_rep\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **We load and inspect the text converted PDF files and some properties below**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will now load one of the PDFs and examine its content. Note that this relies on extracting the content of the PDF as text, which works well for reading regular text but is not as good for tables and pictures. In order to handle tables and figures we typically need to perform pre-processing steps which are more complex and tailored for the specific PDF file type we wish to examine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[103m\u001b[1mNr of pages in pdf document:\u001b[0m 96 \n",
            "\n",
            "The document contains \u001b[31m\u001b[103m\u001b[1m195303\u001b[0m symbols\n",
            "\u001b[31m\u001b[103m\u001b[1mEx text from first page:\u001b[0m\n",
            "\n",
            "IWARNING - READ THIS MANUAL \n",
            "DO NOT OPERATE THIS WHEELCHAIR WITHOUT FIRST READING AND UNDERSTANDING \n",
            "THIS OWNER’S MANUAL. IF YOU ARE UNABLE TO UNDERSTAND THE WARNINGS, \n",
            "CAUTIONS AND INSTRUCTIONS, CONTACT YOUR TiLITE DEALER OR TiLITE CUSTOMER \n",
            "SUPPORT AT (800) 545-2266 BEFORE ATTEMPTING TO USE THIS WHEELCHAIR.  IF  \n",
            "YOU IGNORE THIS WARNING, YOU MAY FALL, TIP OVER OR LOSE CONTROL OF THE \n",
            "WHEELCHAIR AND SERIOUSLY INJURE YOURSELF OR OTHERS OR DAMAGE THE \n",
            "WHEELCHAIR. \n",
            "IWARNING - WHEELCHAIR SELECTION \n",
            "TiLITE MANUFACTURES A WIDE VARIETY OF WHEELCHAIRS TO MEET THE VARIED \n",
            "NEEDS OF WHEELCHAIR USERS. HOWEVER, TiLITE IS NOT YOUR HEALTH CARE \n",
            "ADVISOR, AND WE KNOW NOTHING ABOUT YOUR INDIVIDUAL CONDITION OR NEEDS. \n",
            "THEREFORE, THE FINAL SELECTION OF THE PARTICULAR MODEL, AND HOW IT IS \n",
            "ADJUSTED, AND THE TYPE OF OPTIONS AND ACCESSORIES NECESSARY REST SOLELY \n",
            "WITH YOU, THE WHEELCHAIR USER, AND THE HEALTH CARE PROFESSIONAL THAT IS \n",
            "ADVISING YOU. CHOOSING THE BEST CHAIR AND SETUP FOR YOUR SAFETY DEPENDS \n",
            "ON SUCH THINGS AS: \n",
            " \n",
            "1. YOUR DISABILITY, STRENGTH, BALANCE AND COORDINATION; \n",
            " \n",
            "2. THE TYPES OF HAZARDS YOU MUST OVERCOME IN DAILY USE (WHERE YOU\n",
            " \n",
            "LIVE AND WORK AND OTHER PLACES YOU ARE LIKELY TO USE YOUR CHAIR); AND \n",
            " \n",
            "3. YOUR NEED FOR OPTIONS FOR YOUR SAFETY AND COMFORT (SUCH AS ANTI-\n",
            " \n",
            "TIPS, POSITIONING BELTS OR SPECIAL SEATING SYSTEMS). \n",
            "IF YOU IGNORE THIS WARNING, YOU MAY ENDANGER YOUR HEALTH. \n",
            "IWARNING - TIE-DOWN RESTRAINTS \n",
            "TiLITE RECOMMENDS THAT WHEELCHAIR USERS NOT BE TRANSPORTED IN VEHICLES \n",
            "OF ANY KIND WHILE IN WHEELCHAIRS. AS OF THIS DATE, THE UNITED STATES \n",
            "DEPARTMENT OF TRANSPORTATION HAS NOT APPROVED ANY TIE-DOWN SYSTEM \n",
            "FOR TRANSPORTATION OF A USER WHILE IN A WHEELCHAIR IN A MOVING VEHICLE \n",
            "OF ANY TYPE. IT IS TiLITE’S POSITION THAT USERS OF WHEELCHAIRS SHOULD BE \n",
            "TRANSFERRED INTO APPROPRIATE VEHICLE SEATING FOR TRANSPORTATION AND \n",
            "SHOULD USE THE RESTRAINTS MADE AVAILABLE BY THE AUTO INDUSTRY. TiLITE \n",
            "CANNOT, AND DOES NOT, RECOMMEND ANY WHEELCHAIR TRANSPORTING SYSTEMS. \n",
            "IWARNING - SEATING RESTRAINTS \n",
            "IT IS THE OBLIGATION OF YOUR DEALER AND THE HEALTH CARE PROFESSIONALS \n",
            "WHO ARE ADVISING YOU TO DETERMINE IF YOU REQUIRE A SEATING RESTRAINT \n",
            "OR POSITIONING SYSTEM IN ORDER TO ENSURE THAT YOU CAN SAFELY OPERATE \n",
            "YOUR WHEELCHAIR. SERIOUS INJURY CAN OCCUR IN THE EVENT OF A FALL FROM A \n",
            "WHEELCHAIR.\n",
            " __________________________________________________________________________________________\n",
            "Note: The information contained in this document is subject to change without notice. An updated version of this \n",
            "Owner’s Manual may be available at www.tilite.com \n",
            "__________________________________________________________________________________________ \n",
            "SAVE THIS MANUAL FOR FUTURE REFERENCE \n",
            "ZR Owner’s Manual\n",
            "OM0005_Rev A_ZR\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Here we read in the PDF as text using a file from the helper function text_from_pdf()\n",
        "pdf_text = text_from_pdf(DATAPATH + Patricia_permobil_manual + \".pdf\")\n",
        "\n",
        "# We count and print out the total nr of pages in the PDF\n",
        "print(f\"{Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}Nr of pages in pdf document:{Style.RESET_ALL} {len(pdf_text.keys())} \\n\")\n",
        "\n",
        "# We count and print out the total nr of symbols in the retrieved PDF file\n",
        "concat_text = pdf_dict_to_str(text_from_pdf(DATAPATH + Patricia_permobil_manual + \".pdf\"))\n",
        "print(f\"The document contains {Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}{len(concat_text)}{Style.RESET_ALL} symbols\")\n",
        "\n",
        "# We then print the first page from the PDF\n",
        "print(f\"{Fore.RED + Back.LIGHTYELLOW_EX + Style.BRIGHT}Ex text from first page:{Style.RESET_ALL}\\n\")\n",
        "print(f\"{pdf_text['page_1']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Following section performs chunking of text and creates indexed embeddings for the chunks\n",
        "\n",
        "> **If possible we should probably either remove this section or make it optional**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split the PDF documents into smaller segments of text, referred to as chunks, specifically for the purpose of creating embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OSpFUPzL_tqN"
      },
      "outputs": [],
      "source": [
        "chunked_pdf_files_dict = {}  # Initialize an empty dictionary to store the chunked pdf data.\n",
        "for pdf_file in pdf_files:  # Iterate through each pdf file in the provided list.\n",
        "    pdf_location = DATAPATH + pdf_file + \".pdf\"  # This is the file path for the current pdf file.\n",
        "\n",
        "    # Call the chunk_documents function to split the current pdf document into smaller text chunks.\n",
        "    chunked_pdf = chunk_documents(\n",
        "        documents= TextLoader(file_path=pdf_location, loader=PyPDFLoader),  # Loads the text content of the current pdf file.\n",
        "        TextSplitter=RecursiveCharacterTextSplitter,  # Specify the text splitting mechanism to be used.\n",
        "        chunk_size=512,  # Define the maximum number of characters each chunk should contain. 512 is a commonly used chunk size because many models have a maximum input length of 512 tokens.\n",
        "        chunk_overlap=20,  # Define the number of overlapping characters between adjacent chunks.\n",
        "        separator=None,  # No specific separator is used between chunks.\n",
        "        )\n",
        "\n",
        "    # Store the chunked text data of the current pdf file in the dictionary using the file name as the key.\n",
        "    chunked_pdf_files_dict[pdf_file] = chunked_pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. The assistant at the rear of the chair is in control of this procedure. He or she must tilt the chair back to its balance point on the rear wheels. NEVER attempt to lift a wheelchair by lifting on any removable (detachable) parts, including upholstery and removable push handles or push handle grips. \n",
            "4. The second assistant at the front must firmly grasp a non-detachable part of the front frame (but NOT swing away hangers) with both hands and lift the chair up and over one stair at a time.\n"
          ]
        }
      ],
      "source": [
        "print(chunked_pdf_files_dict[Patricia_permobil_manual][79].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also examine how a chunk of text from a table looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "agreement.  At select business units, we have hired certain highly specialized employees under employment contracts that specify a\n",
            "term of employment, pay and other benefits.\n",
            " \n",
            "Full-T ime Employees at Year-End\n",
            " 2022 2021\n",
            "Inside U.S. 48,200 44,300\n",
            "Outside U.S. 60,900 63,400\n",
            "Total 109,100 107,700\n",
            "By Region:   \n",
            "North America 48,700 44,700\n",
            "EAME 16,900 17,600\n",
            "Latin America 19,100 19,500\n",
            "Asia/Pacific 24,400 25,900\n",
            "Total 109,100 107,700\n"
          ]
        }
      ],
      "source": [
        "print(chunked_pdf_files_dict[caterpillar_10k][104].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Performing the embedding of the chunked PDF documents\n",
        "> **NB: The below code block takes around 100 minutes to complete for the four PDF's (100-150 pages each), so this has been prepared before the lecture. We have kept the code here for those interested, but `DO NOT CHANGE \"FALSE\" TO \"TRUE\" DURING THE LECTURE!` However, feel free to try this out with your own PDF outside of lecture time**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please run the below code snippets as is"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "embed_pdfs = False   # Flag to control whether to do the embedding or not. Note that for the lecture this has already been prepared for you.\n",
        "\n",
        "# For embeddings we use the nr 1 model on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
        "embedding_model = [\"BAAI/bge-large-en-v1.5\"]\n",
        "\n",
        "if embed_pdfs:   # If embedding flag is True, it will proceed with creating the embeddings\n",
        "\n",
        "  embedding_dict = {}  # Dictionary to hold the embeddings\n",
        "  for model_name in embedding_model:\n",
        "    for pdf_file in pdf_files:\n",
        "      print(f\"Creating FAISS embedding for document {pdf_file} using - {model_name}\")\n",
        "      start = timer()  # Starting a timer to measure the embedding process duration\n",
        "      embedding = doc_embedding(embedding_model=model_name)  # Getting the embedding of the document\n",
        "      faiss_index = make_index_FAISS(chunked_documents=chunked_pdf_files_dict[pdf_file], embedding=embedding)  # Creating FAISS index for the embedding. A FAISS index is a data structure used to store vectors and perform searches among them.\n",
        "      end = timer()  # Stop the timer\n",
        "      time_taken=end-start  # Calculate the time taken for the embedding process\n",
        "      print(f\"Done! Embedded vector index created after {time_taken/60:.2f} minutes\")  # Print the time taken for embedding\n",
        "      print(\"*\"*25)\n",
        "      embedding_dict[pdf_file] = faiss_index  # Store the FAISS index in the embedding dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_embeddings = False\n",
        "if save_embeddings:\n",
        "    for pdf_embedding in embedding_dict.keys():\n",
        "        FAISS_folder_name = DATAPATH + f\"FAISS_{pdf_embedding}.faiss\"\n",
        "        embedding_dict[pdf_embedding].save_local(FAISS_folder_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the saved embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The embeddings for this assignments are already saved and we can simply load them from storage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "faiss_embeddings_dict = {}\n",
        "if not save_embeddings:\n",
        "    for pdf_file in pdf_files:\n",
        "        FAISS_folder_name = DATAPATH + f\"FAISS_{pdf_file}.faiss\"\n",
        "        embedding = doc_embedding(embedding_model=embedding_model[0])\n",
        "        faiss_embeddings_dict[pdf_file] = FAISS.load_local(FAISS_folder_name, embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try out making a simple query against the saved embeddings by doing a similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found on page: 78\n",
            "11-4\n",
            "ZR Owner’s Manual OM0005_Rev A_ZRCHAPTER 11: CASTERS AND FORKSNote: TiLite designs its rigid wheelchairs to be flexible for improved maneuverability and increased ride comfort.  \n",
            "However, this flexibility requires that your chair be set up properly.  The following procedure will enable you to set up  your TiLite rigid wheelchair so it will perform to its potential.\n",
            "1. Place the wheelchair on a smooth, level surface with the casters trailing rearward.\n"
          ]
        }
      ],
      "source": [
        "# We choose one of the PDF files, in this example the Permobil Wheelchair manual, and a relevant search query\n",
        "pdf = Patricia_permobil_manual   # Specify the PDF file to be searched.\n",
        "similarity_query = \"What is the procedure for setting up my TiLite rigid wheelchair?\"   # Specify the search query.\n",
        "\n",
        "test = faiss_embeddings_dict[pdf].similarity_search(similarity_query, k=10)  # Similarity search on the specified PDF file for the given query, retrieving the top 10 similar chunks.\n",
        "print(f\"Found on page: {test[0].metadata['page']}\")  # Print the page number where the most similar chunk was found.\n",
        "print(test[0].page_content)  # Print the content of the chunk that was found to be most similar to the query."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Permobil Manual PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the same as already defined above. We put it here again to allow you to change e.g. model or temperature more easily\n",
        "chat_model = ChatOpenAI(\n",
        "    openai_api_key=os.environ['OPENAI_API_KEY'],\n",
        "    model='gpt-3.5-turbo', #'gpt-4'\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# We define a function for more easily call the LLM using the same prompt template but with a different query and context\n",
        "def LLM_with_pdf_context(context: str, query: str, temperature: float=0.0):\n",
        "    \n",
        "    # System prompt\n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful assistant and an expert in investment management.\n",
        "    You are logical, methodical and a problem solving genius. \\\n",
        "    Always find the best and most simple and relevant solution to a problem. \\\n",
        "    Always break down the problem, objects, numbers and logic before starting to solve the problem. \\\n",
        "    Then solve the problem in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt inouts the merged context\n",
        "    user_prompt = f\"\"\"\n",
        "    I need help from an investement management expert. \\\n",
        "    One of our portfolio companies have handed us these brief instruction snippets as\n",
        "    context: {context}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following \n",
        "    question: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context, \\\n",
        "        then return those instructions verbatim and do not try to rephrase them.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "    \n",
        "    response = chat_model(messages, temperature=temperature)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **We should do some testing on different questions here**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We choose one of the PDF files and a relevant search query\n",
        "pdf = Patricia_permobil_manual\n",
        "query = \"What tools are needed for adjusting the Front Seat Height on Slipstream Single-Sided Forks. Also provide the steps for performing the adjustment.\"\n",
        "\n",
        "# Retrieve the most similar chunks related to above search query\n",
        "faiss_index = faiss_embeddings_dict[pdf]  # Access the FAISS index for the specified PDF file.\n",
        "top_hits = similarity_search_FAISS(search_query=query, nr_hits=5, index_store=faiss_index)  # Perform a similarity search in the specified PDF file for the given query, retrieving the top 5 similar chunks.\n",
        "context = \"\".join([document.page_content for document in top_hits])  # Concatenate the content of the top hits to form the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's use all of the retrieved top hits as context when querying the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To adjust the Front Seat Height on Slipstream Single-Sided Forks, the following tools are needed:\n",
            "\n",
            "- 5/8\" Open End Wrench\n",
            "- Screwdriver\n",
            "\n",
            "The steps for performing the adjustment are as follows:\n",
            "\n",
            "1. Remove the caster. Refer to \"Slipstream Single-Sided Forks - Replacing Casters\" on page 11-2 for instructions on how to remove the caster.\n",
            "2. Depending on the fork that came with your chair, you may be able to adjust the Front Seat Height without changing the casters. Follow the procedures under \"Standard Forks - Replacing Casters\" on page 11-1 to mount the casters in the alternative axle holes in the fork.\n",
            "3. Note that the full range of adjustability may not be available with 5\" or 6\" casters. Additional adjustability may be achieved with different forks or casters or with fork stem extensions.\n"
          ]
        }
      ],
      "source": [
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that the instructions are incomplete. The answer we get only contains 3 steps when there is actually 7 steps. This is due to the fact that the most relevant chunk happened to only contain the first part of the full instructions. Since the chunks are small this is something that can happen quite often. In order to try to get a more complete response we can feed not the retrieved and merged chunks as context, but instead feed the whole PDF page of the most relevant hit, assuming it contains more fuller context. We can also do some combination of this and using other chunks if we find that top hit doesn't always work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To adjust the Front Seat Height on Slipstream Single-Sided Forks, the following tools are needed:\n",
            "\n",
            "- 5/8\" Open End Wrench\n",
            "- Screwdriver\n",
            "\n",
            "The steps for performing the adjustment are as follows:\n",
            "\n",
            "1. Remove the caster. Refer to \"Slipstream Single-Sided Forks - Replacing Casters\" on page 11-2 for instructions.\n",
            "2. Using the shaft of the screwdriver, remove the E-Ring by pressing downward across the open portion of the E-Ring. Wear protective eyewear. See Figure 11-3.\n",
            "3. Using the 5/8\" Open End wrench, remove the axle from the Slipstream Single-Sided Fork.\n",
            "4. Place the axle in the alternate axle hole and securely tighten.\n",
            "5. Using the shaft of the screwdriver, replace the E-Ring by pressing downward across the closed portion of the E-Ring, snapping the E-ring into place. See Figure 11-3.\n",
            "6. Replace the caster. Refer to \"Slipstream Single-Sided Forks - Replacing Casters\" on page 11-2 for instructions.\n",
            "7. Follow Steps 1 through 6 on the opposite fork.\n",
            "\n",
            "Please note the following warnings:\n",
            "\n",
            "- Always use identical axle holes on both sides of your chair to ensure stability and safety.\n",
            "- The axles have a locking and sealing coating. If you repeatedly remove and reinstall the axles, reapply the coating after every fourth adjustment to maintain proper functionality and safety.\n"
          ]
        }
      ],
      "source": [
        "# Here we get the page of the top hit and use that whole PDF page as context\n",
        "context_page = f\"\"\"{pdf_text[f'page_{top_hits[0].metadata[\"page\"]}']}\"\"\"\n",
        "\n",
        "# Use the pre-defined function but with the whole context page as context\n",
        "LLM_with_pdf_context(context_page, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this case this worked much better, since the retrieved page actually contained the full instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Electrolux Annual Report PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In this section we will be asking questions to the Electrolux Annual Report. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set our pdf variable to the Electrolux annual report\n",
        "pdf = electrolux_ann_rep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the major events that happened for Electrolux during 2022 are as follows:\n",
            "\n",
            "- Electrolux announced a loss for the fourth quarter of 2022, with an estimated operating income of approximately SEK -2.0bn (0.9), including non-recurring items of SEK -1.4bn (-0.7).\n",
            "- Electrolux decided to discontinue production at the Nyíregyháza factory in Hungary from the beginning of 2024.\n",
            "- Electrolux divested its business in Russia and sold its Russian subsidiary on September 9, 2022, recording a capital loss of SEK 350m.\n",
            "- Electrolux completed a share buyback program, repurchasing a total of 17,369,172 own series B shares for a total amount of SEK 3,032m.\n",
            "\n",
            "Please note that the provided context may contain additional information that could be relevant to Electrolux's major events in 2022.\n"
          ]
        }
      ],
      "source": [
        "#query = \"Explain Electrolux's business in Latin America\"\n",
        "#query = \"Which risk and challenges are mentioned in the report?\"\n",
        "query = \"Which major events happened for Electrolux during 2022?\"\n",
        "\n",
        "# Use our function to retrieve the most similar chunks related to the search query for the given pdf\n",
        "context = get_contexts(pdf=pdf, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "# Use our pre-defined function to get the model response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query for information from the Electrolux Annual Report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the given context, it seems that there is a note repeated multiple times. The note consists of a sequence of numbers from 1 to 31, repeated five times.\n",
            "\n",
            "To answer the question, we need to extract the bulleted list from the given context. Since the context is not provided in a clear format, we need to identify any patterns or structures that might indicate the presence of a bulleted list.\n",
            "\n",
            "Upon analyzing the context, we can observe that the note is repeated five times, each time with the numbers 1 to 31. This repetition suggests that the bulleted list might be the numbers 1 to 31.\n",
            "\n",
            "Therefore, the answer to the question is a bulleted list of the numbers 1 to 31:\n",
            "\n",
            "- 1\n",
            "- 2\n",
            "- 3\n",
            "- 4\n",
            "- 5\n",
            "- 6\n",
            "- 7\n",
            "- 8\n",
            "- 9\n",
            "- 10\n",
            "- 11\n",
            "- 12\n",
            "- 13\n",
            "- 14\n",
            "- 15\n",
            "- 16\n",
            "- 17\n",
            "- 18\n",
            "- 19\n",
            "- 20\n",
            "- 21\n",
            "- 22\n",
            "- 23\n",
            "- 24\n",
            "- 25\n",
            "- 26\n",
            "- 27\n",
            "- 28\n",
            "- 29\n",
            "- 30\n",
            "- 31\n",
            "\n",
            "Please note that this answer assumes that the bulleted list is indeed the numbers 1 to 31, as there is no other information provided in the given context.\n"
          ]
        }
      ],
      "source": [
        "# Write your own query and retrieve search results from that as context for the LLM\n",
        "query = \"...\"   # Replace with your own search query\n",
        "\n",
        "# Get the relevant contexts from the specified PDF based on your query\n",
        "context = get_contexts(pdf=pdf, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "# Call the LLM with the obtained context and your query to get a response\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Caterpillar 10-K PDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set our pdf variable to the Caterpillar 10-K report\n",
        "pdf = caterpillar_10k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the identified risks and challenges mentioned in the report are:\n",
            "\n",
            "- Delays and increased costs resulting from supply chain challenges and availability issues with suppliers.\n",
            "- Freight delays that could impact production in the company's facilities.\n",
            "- Rising costs and the need for appropriate price actions in response.\n",
            "- Fluctuations in freight costs, fuel costs, limitations on shipping and receiving capacity, and other disruptions in the transportation and shipping infrastructure.\n",
            "- Exposure to multiple and potentially conflicting laws, regulations, and policies in different regions and countries.\n",
            "\n",
            "Please note that the identified risks and challenges are mentioned in a paragraph format rather than a bulleted list.\n"
          ]
        }
      ],
      "source": [
        "query = \"Which identified risks and challenges are mentioned in the report?\"\n",
        "\n",
        "context = get_contexts(pdf=pdf, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tables in PDF-reports:**\n",
        "\n",
        "The Caterpillar 10-K report contains multiple tables which may cause a problem when using the text chunks. Here we can try and ask questions about information found in tables in the report. However, depending on the chunks, the model may confuse numbers found in the tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To find the number of full-time employees in Latin America at the end of the year 2022, we need to locate the relevant information in the given context. \n",
            "\n",
            "From the provided context, we can see the following information about full-time employees at year-end:\n",
            "\n",
            "Latin America:\n",
            "2022: 19,100\n",
            "2021: 19,500\n",
            "\n",
            "Based on this information, the number of full-time employees in Latin America at the end of the year 2022 is 19,100.\n",
            "\n",
            "Therefore, the answer to the question is: \n",
            "- Full-Time Employees at Year-End\n",
            "  2022: 19,100\n"
          ]
        }
      ],
      "source": [
        "query = \"How many full-time employees did they have at the end of the year 2022 in Latin America?\"\n",
        "\n",
        "context = get_contexts(pdf=pdf, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by searching with your own query for information from the Caterpillar 10-K report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write your own query and retrieve search results from that as context for the LLM\n",
        "query = \"...\"   # Replace with your own search query\n",
        "\n",
        "context = get_contexts(pdf=pdf, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "LLM_with_pdf_context(context, query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparison: Electrolux Annual Report PDF and Whirlpool 10-K Report PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OBS!! MIKAEL: Jag behövde skriva in temperature = 0.0 i funktionen för att få den att funka, annars tolkades temperature som $.stop. enligt nedan Error message:\n",
        "Den tar även lång tid på sig att köras. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "InvalidRequestError: '$.stop' is invalid. Please check the API reference: https://platform.openai.com/docs/api-reference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "# We define a function for more easily call the LLM using the same prompt template but with a different query and contexts\n",
        "def LLM_with_pdf_context_comparison(company1: str, company2: str, context1: str, context2: str, query: str, temperature: float=0.0):\n",
        "    \n",
        "    system_prompt = f\"\"\"\n",
        "    Ignore all previous instructions. You are a helpful assistant and an expert in investment management.\n",
        "    You are logical, methodical and a problem solving genius. \\\n",
        "    Always find the best and most simple and relevant solution to a problem. \\\n",
        "    Always break down the problem, objects, numbers and logic before starting to solve the problem. \\\n",
        "    Then solve the problem in a step-by-step manner.\n",
        "    \"\"\"\n",
        "\n",
        "    # This prompt inouts the merged context\n",
        "    user_prompt = f\"\"\"\n",
        "    I need help from an investement management expert. \\\n",
        "    Two of our portfolio companies have handed us these brief instruction snippets as\n",
        "    contexts. Contexts for {company1}: {context1}, and for {company2}: {context2}\n",
        "\n",
        "    Use only the above context and nothing else to answer the following \n",
        "    question: {query}\n",
        "    If the answer is provided in the form of a bulleted list within the context, \\\n",
        "        then return those instructions verbatim and do not try to rephrase them.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        SystemMessage(content=system_prompt),\n",
        "        HumanMessage(content=user_prompt),\n",
        "    ]\n",
        "\n",
        "    response = chat_model(messages, temperature=0.0)\n",
        "    print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use queries to compare the information in different pdf:s by creating separate contexts. Here, we compare the information in the Electrolux Annual report and Whirlpool 10-K report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's make a query which we would like to pose to both the Caterpillar and the Electrolux PDF files\n",
        "query = \"Which key business strategies were mentioned in the report?\"\n",
        "\n",
        "# Using the above query, we can fetch relevant contexts from both PDF files\n",
        "context_whirlpool = get_contexts(pdf=whirlpool_10k, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "context_electrolux = get_contexts(pdf=electrolux_ann_rep, query=query, faiss_embeddings_dict=faiss_embeddings_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Given the above retrieved contexts we can now ask a comparative question between the companies\n",
        "chat_query = \"This is the key business strategies for Whirlpool and Electrolux. Compare them. Which strategies are similar and which are different?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The key business strategies for Whirlpool and Electrolux can be compared as follows:\n",
            "\n",
            "Similar strategies:\n",
            "- Both companies focus on investing in businesses that support high growth and high margins.\n",
            "- They both emphasize the importance of digitalization and automation in providing cost-competitive products with high quality.\n",
            "- Both companies target the middle class as a driving force for market growth in emerging markets.\n",
            "- They both consider industry trends and market intelligence to develop an understanding of the prevailing business context.\n",
            "- Both companies have financial targets for profitable growth and prioritize achieving these targets.\n",
            "- They both aim to create and sustain value through their business strategies.\n",
            "\n",
            "Different strategies:\n",
            "- Whirlpool highlights three strong pillars: small appliances, major appliances in the Americas and India, and commercial appliances. Electrolux does not mention specific pillars but focuses on driving sustainable consumer experience innovation and increasing efficiency.\n",
            "- Whirlpool mentions potential challenges such as competition, changing consumer preferences, supply chain constraints, inflationary pressures, currency fluctuations, and geopolitical uncertainty. Electrolux does not explicitly mention these challenges.\n",
            "- Electrolux mentions a sustainability framework called \"For the Better 2030\" with ten defined areas, while Whirlpool does not mention a specific sustainability framework.\n",
            "\n",
            "Please note that the above comparison is based on the provided context and may not capture all aspects of the companies' strategies.\n"
          ]
        }
      ],
      "source": [
        "# Use our pre-defined function together with the names of the companies and their context\n",
        "LLM_with_pdf_context_comparison(\"Whirlpool\", \"Electrolux\", context_whirlpool, context_electrolux, chat_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Try it out yourself:**\n",
        "\n",
        "Try it out by telling your model what information from the reports you want to compare, e.g., their sustainability strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Based on the provided context, the following are the instructions for Whirlpool and Electrolux:\n",
            "\n",
            "Whirlpool:\n",
            "- Whirlpool uses Design for Sustainability principles in their global platforms and connects product sustainability directly with their business goals.\n",
            "- They are committed to innovating for a new generation of consumers and have a world-class innovation pipeline.\n",
            "- Whirlpool has set rigorous science-based targets for greenhouse gas reductions and other sustainability goals.\n",
            "- They believe that an environmentally sustainable Whirlpool is a more competitive company in the long term.\n",
            "- Whirlpool is committed to protecting the environment, supporting employee growth, and making their homes, communities, and operations better today and in the future.\n",
            "\n",
            "Electrolux:\n",
            "- Electrolux believes that sustainability leadership is crucial for long-term profitable growth.\n",
            "- They have a consistent approach to sustainability in the countries where they operate.\n",
            "- Sustainability is a key business driver for Electrolux, contributing to improved margins and strengthening relations with stakeholders.\n",
            "- They aim to have a climate neutral value chain and offer sustainable products that contribute to retailer sustainability goals.\n",
            "- Electrolux recognizes the importance of sustainability performance and is acknowledged as a sustainability leader in the household durables industry.\n",
            "\n",
            "In summary, both Whirlpool and Electrolux prioritize sustainability and believe it is essential for their long-term success. They have set targets for reducing environmental impacts and offer sustainable products. Both companies aim to be leaders in sustainability within their respective industries.\n"
          ]
        }
      ],
      "source": [
        "# Write your own query on what you like to pose to both the Caterpillar and the Electrolux PDF files\n",
        "query = \"...\"\n",
        "\n",
        "# Using the above query, we can fetch relevant contexts from both PDF files\n",
        "context_whirlpool = get_contexts(pdf=whirlpool_10k, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "context_electrolux = get_contexts(pdf=electrolux_ann_rep, query=query, faiss_embeddings_dict=faiss_embeddings_dict)\n",
        "\n",
        "# Change this query to ask the model to compare ... for the companies.\n",
        "chat_query = \"This is the ... for Whirlpool and Electrolux. Compare them.\"\n",
        "\n",
        "# Use our pre-defined function together with the names of the companies and their context\n",
        "LLM_with_pdf_context_comparison(\"Whirlpool\", \"Electrolux\", context_whirlpool, context_electrolux, chat_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check accuracy of tests (OPTIONAL - MOSTLY FOR DEVELOPMENT TESTING AND WILL BE MODIFIED OR REMOVED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Below code block is implemented to do some tests for checking that similarity works as expected**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UA5Rc1eH61Ia",
        "outputId": "c118bbd9-6516-40ae-f292-2b261d34243d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating FAISS embedding using - BAAI/bge-large-en-v1.5\n",
            "Done!\n",
            " Embedded vector index created after 2.85235 [s]\n",
            "Starting search using - BAAI/bge-large-en-v1.5\n",
            "Done!\n",
            " Searched through 1 queries in time: 0.30055 [s]\n",
            "accuracy: 100.00%\n",
            "top5_accuracy: 100.00%, hit at vector nr 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# loop through pdf, get a text snippet per page and make queries to check that the right page is retrieved\n",
        "# Only included for testing that exact queries are working as expected\n",
        "# This uses the chunking performed in previous section\n",
        "queries = [chunked_pdf[idx].page_content[50:150] for idx in range(len(chunked_pdf))]\n",
        "metadatas = [chunked_pdf[idx].metadata[\"page\"] for idx in range(len(chunked_pdf))]\n",
        "\n",
        "\n",
        "# if testing only one query for similarity\n",
        "page_id = 78\n",
        "exact_query = \"The following procedure will enable you to set up your TiLite rigid wheelchair so it will perform to its potential.\"\n",
        "similarity_query = \"What is the procedure for setting up my TiLite rigid wheelchair?\"\n",
        "\n",
        "queries = [similarity_query]\n",
        "\n",
        "\n",
        "# For embeddings we use the nr 1 model on the MTEB leaderboard at https://huggingface.co/spaces/mteb/leaderboard\n",
        "# Must be same as used for the saved embeddings\n",
        "embedding_model = [\"BAAI/bge-large-en-v1.5\"]\n",
        "\n",
        "scores = {}\n",
        "for model in embedding_model:\n",
        "  print(f\"Creating FAISS embedding using - {model}\")\n",
        "  start = timer()\n",
        "  embedding = doc_embedding(embedding_model=model)\n",
        "  faiss_index = faiss_embeddings_dict[Patricia_permobil_manual]\n",
        "  end = timer()\n",
        "  time_taken=end-start\n",
        "  print(f\"Done!\\n Embedded vector index created after {time_taken:.5f} [s]\")\n",
        "  accuracy = 0\n",
        "  top5_accuracy = 0\n",
        "  cnt=0\n",
        "  print(f\"Starting search using - {model}\")\n",
        "  start = timer()\n",
        "  for search_query in queries:\n",
        "    top_hits = similarity_search_FAISS(search_query=search_query, nr_hits=5, index_store=faiss_index)\n",
        "\n",
        "    if top_hits[0].metadata[\"page\"] == page_id: #metadatas[cnt]:\n",
        "      accuracy+=1\n",
        "    is_in_top_5, vec_hit_nr = search_result_is_in_top_5(docs=top_hits, id=page_id) #id=metadatas[cnt])\n",
        "    if is_in_top_5:\n",
        "      top5_accuracy+=1\n",
        "    cnt+=1\n",
        "  end = timer()\n",
        "  time_taken=end-start\n",
        "  print(f\"Done!\\n Searched through {len(queries)} queries in time: {time_taken:.5f} [s]\")\n",
        "  if len(queries) > 1:\n",
        "    accuracy = accuracy/len(queries)\n",
        "    top5_accuracy = top5_accuracy/len(queries)\n",
        "  scores[\"model\"] = model\n",
        "  scores[\"accuracy\"] = accuracy\n",
        "  scores[\"top5_accuracy\"] = top5_accuracy\n",
        "  print(f\"accuracy: {accuracy*100:.2f}%\")\n",
        "  print(f\"top5_accuracy: {top5_accuracy*100:.2f}%, hit at vector nr {vec_hit_nr}\")\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'model': 'BAAI/bge-large-en-v1.5', 'accuracy': 1, 'top5_accuracy': 1}"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below code snippet can be used during testing to make sure that we retrieve the correct pages for some example questions for which we know the correct answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating FAISS embedding using - BAAI/bge-large-en-v1.5\n",
            "Done!\n",
            " Embedded vector index created after 2.60682 [s]\n",
            "Starting search using - BAAI/bge-large-en-v1.5\n",
            "Done!\n",
            " Searched through 1 queries in time: 0.12784 [s]\n",
            "accuracy: 100.00%\n",
            "top5_accuracy: 100.00%, hit at vector nr 0\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# RAG test\n",
        "\n",
        "\n",
        "#page_id = 73  # page index starts at 0\n",
        "#page_id = 18  # page index starts at 0\n",
        "page_id = 77  # page index starts at 0\n",
        "#search_query = \"What tools are needed to remove the wheel lock handle?\"\n",
        "#search_query = \"What should you do to reduce the risk of the wheelchair tipping over?\"\n",
        "search_query = \"What tools are needed for adjusting the Front Seat Height on Slipstream Single-Sided Forks. Also provide the steps for performing the adjustment.\"\n",
        "queries = [search_query]\n",
        "\n",
        "contexts = []\n",
        "scores = {}\n",
        "for model in embedding_model:\n",
        "  print(f\"Creating FAISS embedding using - {model}\")\n",
        "  start = timer()\n",
        "  embedding = doc_embedding(embedding_model=model)\n",
        "  #faiss_index = embedding_dict[model_name]\n",
        "  faiss_index = faiss_embeddings_dict[Patricia_permobil_manual]\n",
        "  end = timer()\n",
        "  time_taken=end-start\n",
        "  print(f\"Done!\\n Embedded vector index created after {time_taken:.5f} [s]\")\n",
        "  accuracy = 0\n",
        "  top5_accuracy = 0\n",
        "  cnt=0\n",
        "  print(f\"Starting search using - {model}\")\n",
        "  start = timer()\n",
        "  for search_query in queries:\n",
        "    top_hits = similarity_search_FAISS(search_query=search_query, nr_hits=5, index_store=faiss_index)\n",
        "    context = \"\".join([document.page_content for document in top_hits])\n",
        "    contexts.append(context)\n",
        "    if top_hits[0].metadata[\"page\"] == page_id: #metadatas[cnt]:\n",
        "      accuracy+=1\n",
        "    is_in_top_5, vec_hit_nr = search_result_is_in_top_5(docs=top_hits, id=page_id) #id=metadatas[cnt])\n",
        "    if is_in_top_5:\n",
        "      top5_accuracy+=1\n",
        "    cnt+=1\n",
        "  end = timer()\n",
        "  time_taken=end-start\n",
        "  print(f\"Done!\\n Searched through {len(queries)} queries in time: {time_taken:.5f} [s]\")\n",
        "  if len(queries) > 1:\n",
        "    accuracy = accuracy/len(queries)\n",
        "    top5_accuracy = top5_accuracy/len(queries)\n",
        "  scores[\"model\"] = model\n",
        "  scores[\"accuracy\"] = accuracy\n",
        "  scores[\"top5_accuracy\"] = top5_accuracy\n",
        "  print(f\"accuracy: {accuracy*100:.2f}%\")\n",
        "  print(f\"top5_accuracy: {top5_accuracy*100:.2f}%, hit at vector nr {vec_hit_nr}\")\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "qIcs3MdgdYg7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have 1 contexts, merged from 5 retrieved snippets, which can be fed to our LLM\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'11-3\\nZR Owner’s Manual OM0005_Rev A_ZRCHAPTER 11: CASTERS AND FORKSAdjusting the Front Seat Height o'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(f\"We have {len(contexts)} contexts, merged from {len(top_hits)} retrieved snippets, which can be fed to our LLM\")\n",
        "contexts[0][:100]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### End of the notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **NB: After completed course we can flush and unmount drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First flush and unmount drive after we are done,\n",
        "# but to re-mount with new login we may need to remove dir manually first\n",
        "if use_drive:\n",
        "  drive.flush_and_unmount()\n",
        "  !rm -rf /content/drive\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "FkZyvXSmxl3S",
        "uczbePAtomja",
        "S0W6WCuAdIJm",
        "Tad6kXJ_xtj6",
        "BUoFq1J5nTOE",
        "q97YM4YIncHT",
        "z9ins43Unf0Y"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "cb01f1dbab416340528beb9003d373e701839649ec4e19c51caaaa46d79e9db4"
    },
    "kernelspec": {
      "display_name": "Python 3.11.5 ('pe_short_course')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
